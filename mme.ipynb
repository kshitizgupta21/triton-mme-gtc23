{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7210102-aba1-42f4-a5f7-90bac19ac6b3",
   "metadata": {},
   "source": [
    "# Deploying Models from Multiple Frameworks on GPU using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c5177",
   "metadata": {},
   "source": [
    "Amazon SageMaker multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now, customers can deploy 1000s of deep learning models on GPUs behind one SageMaker endpoint. MME will run multiple models on a GPU, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve the best price performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308d276",
   "metadata": {},
   "source": [
    "In this section we show how MME on GPU allows you to deploy ML models from different frameworks like PyTorch, TensorRT, TensorFlow, ONNX, etc. In this example, we show the deployment of PyTorch and TensorRT DistilBERT models on same GPU using SageMaker MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc345c35",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "Set conda_python3 kernel when prompted to set the kernel for this notebook. This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5.xlarge`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483eb5-f1ee-43ac-8f8b-3bc2a02a1fc3",
   "metadata": {},
   "source": [
    "### Installs <a class=\"anchor\" id=\"installs-and-set-up\"></a>\n",
    "\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2956ae-e471-401e-9eb7-87a69df9c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip boto3 sagemaker awscli tritonclient[http] transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b537a3-72b8-40d9-aed4-415efd008d09",
   "metadata": {},
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92279c0c-cd98-4928-8425-3530697fb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from utils import print_safe\n",
    "\n",
    "# sagemaker variables\n",
    "prefix = \"nlp-mme-gpu\"\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dede4-e603-4e48-adb8-2d5a55b8f0d0",
   "metadata": {},
   "source": [
    "## Creating Model Artifacts <a class=\"anchor\" id=\"pytorch-efficientnet-model\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f2e9a",
   "metadata": {},
   "source": [
    "This section presents overview of steps to prepare HuggingFace DistilBERT models to be deployed on SageMaker MME using Triton Inference server model configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb7345-7bee-4ade-9213-1bd7082ca900",
   "metadata": {},
   "source": [
    "### Prepare PyTorch Model  <a class=\"anchor\" id=\"create-pytorch-model\"></a>\n",
    "\n",
    "\n",
    "* We load pre-trained PyTorch [Huggingface DistilBERT model](https://huggingface.co/bergum/xtremedistil-emotion) that was finetuned on emotion classification task. \n",
    "* Export it to Torchscript serialized format\n",
    "\n",
    "To perform these steps, we will be using our [pt_exporter.py](./workspace/pt_exporter.py) script and running it within the [PyTorch NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87837e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./workspace/pt_exporter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./workspace/pt_exporter.py\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bergum/xtremedistil-emotion\", torchscript=True)\n",
    "\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "bs = 224\n",
    "seq_len = 128\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.ones(bs, seq_len, dtype=torch.int).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "\n",
    "traced_model = torch.jit.trace(model, dummy_inputs)\n",
    "torch.jit.save(traced_model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebcf08",
   "metadata": {},
   "source": [
    "Run the cell below to finish preparing the PyTorch DistilBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae16a-1bde-422b-9635-5b6e005e59c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108feee3",
   "metadata": {},
   "source": [
    "#### Setup PyTorch Model Repository\n",
    "\n",
    "Now that model artifact is ready we need to set up a model repository containing the model artifact we want to serve along with a [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) file i.e. `config.pbtxt`. This is the expected structure of the model repository:\n",
    "```\n",
    "xdistilbert_pt\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.pt\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af9227",
   "metadata": {},
   "source": [
    "#### PyTorch Model configuration <a class=\"anchor\" id=\"create-pytorch-model-config\"></a>\n",
    "\n",
    "Model configuration file `config.pbtxt` contains the following:  \n",
    "- `name`: xdistilbert_pt\n",
    "- `backend`: pytorch\n",
    "- `max_batch_size`: maximum batch size 224 that the model supports\n",
    "- `input` and `output` tensor shapes with the `data_type` \n",
    "\n",
    "Additionally, you can specify [instance_group](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#instance-groups) and [dynamic_batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batcher) properties to achieve high performance inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a21ac8-012c-4e6c-8d17-27838295f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_pt/1\n",
    "!cp workspace/model.pt model_repository/xdistilbert_pt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b45c395e-c52a-4a28-a18b-7baa746fc1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_repository/xdistilbert_pt/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repository/xdistilbert_pt/config.pbtxt\n",
    "backend: \"pytorch\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "    {\n",
    "    name: \"INPUT__2\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87036cb6-f830-495a-b2cc-9651ec93fe06",
   "metadata": {},
   "source": [
    "### Prepare TensorRT Model <a class=\"anchor\" id=\"create-tensorrt-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca98ddb-d90b-4095-b310-825d09774954",
   "metadata": {},
   "source": [
    "- We load pre-trained xdistilbert PyTorch model from Huggingface\n",
    "- Convert to onnx representation using torch onnx exporter.\n",
    "- Use TensorRT `trtexec` bash command to create the TensorRT model plan\n",
    "\n",
    "To perform these steps, we will be running the [generate_model_trt.sh](./workspace/generate_model_trt.sh) script inside the [PyTorch NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c98b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./workspace/generate_model_trt.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./workspace/generate_model_trt.sh\n",
    "\n",
    "echo \"Installing Transformers...\"\n",
    "pip -q install transformers[onnx]\n",
    "\n",
    "echo \"Exporting model to ONNX...\"\n",
    "python -m transformers.onnx --model=bergum/xtremedistil-emotion \\\n",
    "                            --feature=sequence-classification /workspace/onnx/\n",
    "\n",
    "export CUDA_MODULE_LOADING=LAZY\n",
    "echo \"Converting ONNX Model to TensorRT FP16 Plan...\"\n",
    "trtexec --onnx=/workspace/onnx/model.onnx \\\n",
    "        --saveEngine=/workspace/model.plan \\\n",
    "        --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "        --optShapes=input_ids:16x128,attention_mask:16x128,token_type_ids:16x128 \\\n",
    "        --maxShapes=input_ids:224x128,attention_mask:224x128,token_type_ids:224x128 \\\n",
    "        --fp16 \\\n",
    "        --verbose \\\n",
    "        --memPoolSize=workspace:14000 | tee conversion_trt.txt\n",
    "\n",
    "echo \"Finished exporting all models...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3925c",
   "metadata": {},
   "source": [
    "Execute the cell below to finish preparing the TensorRT DistilBERT model.\n",
    "\n",
    "**Note**:\n",
    "This TensorRT optimization step takes around 10 minutes to complete. While the step is running, feel free to watch this [video](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41306/?start=349) or take a look at the logs in the below cell to see the various TensorRT optimizations like Layer & Tensor Fusion and Reduced Mixed Precision in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefd65d-f654-44b6-9569-0a1d5597a7f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_trt.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45708396",
   "metadata": {},
   "source": [
    "#### Setup TensorRT Model Repository\n",
    "\n",
    "Similar to PyTorch model, this is the expected structure of the TensorRT model repository:\n",
    "```\n",
    "xdistilbert_trt\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.plan\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61185351",
   "metadata": {},
   "source": [
    "#### TensorRT Model configuration <a class=\"anchor\" id=\"create-pytorch-model-config\"></a>\n",
    "\n",
    "Model configuration file `config.pbtxt` contains the following  \n",
    "- `name`: xdistilbert_trt\n",
    "- `backend`: tensorrt\n",
    "- `max_batch_size`: maximum batch size 224 that the model supports\n",
    "- `input` and `output` tensor shapes with the `data_types`\n",
    "\n",
    "Additionally, you can specify [instance_group](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#instance-groups) and [dynamic_batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batcher) properties to achieve high performance inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610b257-20d2-4f16-8f05-e482d42608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_trt/1/\n",
    "!cp workspace/model.plan model_repository/xdistilbert_trt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d9fdb-b6d0-40fd-bd29-96a53a6a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repository/xdistilbert_trt/config.pbtxt\n",
    "name: \"xdistilbert_trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"token_type_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d67ee3-e301-4edb-a6d4-d0b1f35ad2b3",
   "metadata": {},
   "source": [
    "## Export model artifacts to S3 <a class=\"anchor\" id=\"export-to-s3\"></a>\n",
    "\n",
    "Next, we will package our models as `*.tar.gz` files for uploading to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5022a48f-5d52-4697-8b12-f00fc297f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model_file_name = \"xdistilbert_pt.tar.gz\"\n",
    "!tar -C model_repository -czf $pytorch_model_file_name xdistilbert_pt\n",
    "model_uri_pt = sagemaker_session.upload_data(path=pytorch_model_file_name, key_prefix=prefix)\n",
    "print_safe(f\"PyTorch Model S3 location: {model_uri_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296c069a-2fad-4e22-891a-cf0a05920327",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorrt_model_file_name = \"xdistilbert_trt.tar.gz\"\n",
    "!tar -C model_repository -czf $tensorrt_model_file_name xdistilbert_trt\n",
    "model_uri_trt = sagemaker_session.upload_data(path=tensorrt_model_file_name, key_prefix=prefix)\n",
    "print_safe(f\"TensorRT Model S3 location: {model_uri_trt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8f41-08e5-4999-98e7-92ee2852940d",
   "metadata": {},
   "source": [
    "## Setup Multi-Model Endpoint <a class=\"anchor\" id=\"deploy-models-with-mme\"></a>\n",
    "\n",
    "We will now setup Multi-Model Endpoint on GPU where we can deploy our DistilBERT PyTorch and TensorRT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379000a-3116-4ed4-851b-a041e4cfb3bf",
   "metadata": {},
   "source": [
    "### SageMaker Triton Container Image\n",
    "\n",
    "First we define that we will be using SageMaker Triton container image which supports deploying multi-model endpoints with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25758107-1d7a-4085-8a6a-908844f03ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf251ac",
   "metadata": {},
   "source": [
    "### Define the serving container  <a class=\"anchor\" id=\"define-container-def\"></a>\n",
    "\n",
    "Next, we define the serving container\n",
    "* In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load  and serve predictions. \n",
    "* Set `Mode` to `MultiModel` to indicate SageMaker should create the endpoint with MME specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53b00914-a237-4455-a08c-d79b3715cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "container = {\"Image\": mme_triton_image_uri, \"ModelDataUrl\": model_data_url, \"Mode\": \"MultiModel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e28483-e6cf-4769-b6f3-4ccaa61471ec",
   "metadata": {},
   "source": [
    "### Create SageMaker model <a class=\"anchor\" id=\"create-mme-model-obj\"></a>\n",
    "\n",
    "We start off by creating a sagemaker model from the model files we uploaded to s3 in the previous step. We do this using the SageMaker boto3 client and [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the `create model` API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2077c860-1ef6-42f6-b10b-c77e075d2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:############:model/nlp-mme-gpu-mdl-2023-02-01-04-05-24\n"
     ]
    }
   ],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{prefix}-model-{ts}\"\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print_safe(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8e07e-1410-471e-9c2c-10e0c23db00f",
   "metadata": {},
   "source": [
    "### Define configuration for the MME<a class=\"anchor\" id=\"config-mme\"></a>\n",
    "\n",
    "Next, we create a multi-model endpoint configuration using [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) boto3 API. Specify an accelerated GPU computing instance in InstanceType **(we will use the same instance type that we are using to host our SageMaker Notebook)**. We recommend configuring your endpoints with at least two instances with real-life use-cases. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b55e160-8aa0-4ef7-84f5-cb975df99f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:############:endpoint-config/nlp-mme-gpu-epc-2023-02-01-04-05-24\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": ml.g5.xlarge,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print_safe(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943aee-7692-4048-9a47-256d8e21c112",
   "metadata": {},
   "source": [
    "### Create MME  <a class=\"anchor\" id=\"create-mme\"></a>\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1fc49275-e1c2-46c3-a8be-9e193c271262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:############:endpoint/nlp-mme-gpu-ep-2023-02-01-04-05-24\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print_safe(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404e50c-9f66-4b27-9b9a-714f474cc1d9",
   "metadata": {},
   "source": [
    "### Describe MME <a class=\"anchor\" id=\"describe-mme\"></a>\n",
    "\n",
    "Now, we check the status of the endpoint using `describe_endpoint`. This step will take about 7 mins to complete and you should see `Status: InService` message before you proceed to next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1d7ac9f8-8ba9-4f37-a2cb-7dc6d428885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:############:endpoint/nlp-mme-gpu-ep-2023-02-01-04-05-24\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print_safe(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a0ed6-ff2d-46cb-a396-4beef96b58d9",
   "metadata": {},
   "source": [
    "## Preparing inference payload <a class=\"anchor\" id=\"helper-functions\"></a>\n",
    "\n",
    "The following methods help us tokenize text and perform some postprocessing on logits to get final classification prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "daae74d9-1058-4686-9f36-e297277d26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/ec2-user/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237efe9ada024f7fab983ae5fadf52f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"emotion\")\n",
    "tokenizer_name = \"bergum/xtremedistil-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_text(tokenizer, text):\n",
    "    MAX_LEN = 128\n",
    "    tokenized_text = tokenizer(text, padding='max_length', max_length=MAX_LEN, add_special_tokens=True, return_tensors='np')\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask, tokenized_text.token_type_ids\n",
    "\n",
    "def get_random_text():\n",
    "    rand_i = random.randint(0, 2000)\n",
    "    text = dataset[\"test\"][\"text\"][rand_i]\n",
    "    return text\n",
    "\n",
    "def logits2prediction(logits):\n",
    "    CLASSES = [\"SADNESS\", \"JOY\", \"LOVE\", \"ANGER\", \"FEAR\", \"SURPRISE\"]\n",
    "    predictions = []\n",
    "    for i in range(len(logits)):\n",
    "        pred_class_idx = np.argmax(logits[i])\n",
    "        predictions.append(CLASSES[pred_class_idx])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def9a67",
   "metadata": {},
   "source": [
    "While Triton on SageMaker supports json payload format Here we use `binary+json` as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the `binary+json` format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `application/vnd.sagemaker-triton.binary+json;json-header-size={}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88f24816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tokenized_text_binary(text, input_names, output_names):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_ids, attention_mask, token_type_ids = tokenize_text(tokenizer, text)\n",
    "    inputs.append(httpclient.InferInput(input_names[0], input_ids.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], attention_mask.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[2], token_type_ids.shape, \"INT32\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True)\n",
    "    inputs[2].set_data_from_numpy(token_type_ids.astype(np.int32), binary_data=True)\n",
    "    \n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "def get_tokenized_text_binary_pt(text):\n",
    "    return _get_tokenized_text_binary(text, [\"INPUT__0\", \"INPUT__1\", \"INPUT__2\"], [\"OUTPUT__0\"])\n",
    "\n",
    "\n",
    "def get_tokenized_text_binary_trt(text):\n",
    "    return _get_tokenized_text_binary(text, [\"input_ids\", \"attention_mask\", \"token_type_ids\"], [\"logits\"])\n",
    "\n",
    "def read_response(response, output_name):\n",
    "    # Parse json header size length from the response\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response['Body'].read(), header_length=int(header_length_str))\n",
    "    logits = result.as_numpy(output_name)\n",
    "    prediction = logits2prediction(logits)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887d53d-d9b4-41bf-ab74-07a86cecd04c",
   "metadata": {},
   "source": [
    "## Invoking Models on Multi-Model Endpoint\n",
    "\n",
    "Once the endpoint is successfully created, we can send inference request to multi-model endpoint using `invoke_enpoint` API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a26520",
   "metadata": {},
   "source": [
    "### Invoke TensorRT Model <a class=\"anchor\" id=\"invoke-tensorrt-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97475d",
   "metadata": {},
   "source": [
    "Notice the higher latencies on the first invocation of any given model. This is due to the time it takes SageMaker to download the model to the Endpoint instance and then load the model into the inference container. Subsequent invocations of the same model take advantage of the model already being loaded into the inference container and so are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0a84bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i suspect his reasoning may simply be to lull apple into feeling complacent\n",
      "\n",
      "prediction: ['JOY'], took 2857 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_trt(sample_text)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_trt.tar.gz')\n",
    "duration = time.time() - start_time\n",
    "\n",
    "output_name = 'logits'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "386a5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i sat up to embrace them and realised that two hours spent shaking my thang in an eighties bar celebrating the fact i am one year closer to death had left my ageing body feeling punished and my normally pink feet blackened\n",
      "\n",
      "prediction: ['SADNESS'], took 12 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_trt(sample_text)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_trt.tar.gz')\n",
    "duration = time.time() - start_time\n",
    "\n",
    "output_name = 'logits'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a05c-ae58-484c-9fed-4c9584795c7f",
   "metadata": {},
   "source": [
    "### Invoke PyTorch Model <a class=\"anchor\" id=\"invoke-pytorch-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f4ab429b-5b9c-4edd-9ff7-5f974dd35924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i feel reassured that the county government in my county takes the murder of an illegal immigrant in a back alley seriously enough to prosecute someone years later\n",
      "\n",
      "prediction: ['JOY'], took 12 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_pt(sample_text)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_pt.tar.gz')\n",
    "\n",
    "output_name = 'OUTPUT__0'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75ea8bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i feel so blessed to be able to share it with you all\n",
      "\n",
      "prediction: ['LOVE'], took 12 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_pt(sample_text)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_pt.tar.gz')\n",
    "\n",
    "output_name = 'OUTPUT__0'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa3d1a-c0f2-4809-a7e2-71056556c864",
   "metadata": {},
   "source": [
    "# Deploying Hundreds of Models to GPUs using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f67b3-0eb6-4ead-8851-c89b2628cb1c",
   "metadata": {},
   "source": [
    "Let's say you are trying to deploy 1000 customer-specific distilBERT models which are a mixture of frequently and infrequently accessed models coming from different frameworks like PyTorch, TensorFlow, ONNX, TensorRT.\n",
    "\n",
    "Deploying these 1000 models on GPU instances like `g5.xlarge` using dedicated Single-Model Endpoints would take ~1000 instances.\n",
    "\n",
    "By deploying these models behind a Multi-Model endpoint on GPUs you can end up using 100x less instances. Thus reducing costs by **100x**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedb827-b9e7-4b54-98f4-f3effa813bc5",
   "metadata": {},
   "source": [
    "## Dynamically adding models to an existing endpoint\n",
    "\n",
    "Itâ€™s easy to deploy a new model to an existing multi-model endpoint. With the endpoint already running, copy a new set of model artifacts to the same S3 location you set up earlier. Client applications are then free to request predictions from that target model, and Amazon SageMaker handles the rest. \n",
    "\n",
    "This step below will take around few minutes to complete as we are copying 300 files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "49c2410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_pt.tar.gz to s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer302.tar.gz\n"
     ]
    }
   ],
   "source": [
    "customer_model_name = f\"xdistilbert_customer302.tar.gz\"\n",
    "model_copy = f\"{model_data_url}{customer_model_name}\"\n",
    "!aws s3 cp $model_data_url$pytorch_model_file_name $model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7fd99243-9f2e-4954-8ec8-d9b9c8301ddb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "num_models = 100\n",
    "for i in range(num_models):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    model_copy = f\"{model_data_url}{customer_model_name}\"\n",
    "    !aws s3 cp $model_data_url$pytorch_model_file_name $model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f72ebb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_pt.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "model_name=\"xdistilbert_pt.tar.gz\"\n",
    "!aws s3 rm $model_data_url$model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9c28257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer0.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer10.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer11.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer1.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer18.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer19.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer14.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer16.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer12.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer13.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer15.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer23.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer20.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer21.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer24.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer28.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer29.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer30.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer32.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer31.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer17.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer33.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer36.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer3.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer35.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer37.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer38.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer34.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer25.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer39.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer40.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer4.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer44.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer42.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer43.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer45.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer41.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer47.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer5.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer52.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer46.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer53.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer54.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer56.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer51.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer55.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer49.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer50.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer58.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer48.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer59.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer57.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer60.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer62.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer61.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer64.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer67.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer66.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer7.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer68.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer6.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer69.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer72.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer74.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer73.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer75.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer70.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer77.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer79.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer8.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer71.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer81.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer80.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer85.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer76.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer84.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer82.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer83.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer87.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer9.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer89.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer88.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer90.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer92.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer86.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer94.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer96.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer95.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer98.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer97.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer93.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer26.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer99.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer27.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_pt.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer22.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer2.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer63.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer65.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer78.tar.gz\n",
      "delete: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_customer91.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm $model_data_url --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "82ccbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 04:16:56   46621941 xdistilbert_customer0.tar.gz\r\n",
      "2023-02-01 04:16:57   46621941 xdistilbert_customer1.tar.gz\r\n",
      "2023-02-01 04:17:06   46621941 xdistilbert_customer10.tar.gz\r\n",
      "2023-02-01 04:17:07   46621941 xdistilbert_customer11.tar.gz\r\n",
      "2023-02-01 04:17:08   46621941 xdistilbert_customer12.tar.gz\r\n",
      "2023-02-01 04:17:09   46621941 xdistilbert_customer13.tar.gz\r\n",
      "2023-02-01 04:17:11   46621941 xdistilbert_customer14.tar.gz\r\n",
      "2023-02-01 04:17:12   46621941 xdistilbert_customer15.tar.gz\r\n",
      "2023-02-01 04:17:13   46621941 xdistilbert_customer16.tar.gz\r\n",
      "2023-02-01 04:17:13   46621941 xdistilbert_customer17.tar.gz\r\n",
      "2023-02-01 04:17:15   46621941 xdistilbert_customer18.tar.gz\r\n",
      "2023-02-01 04:17:16   46621941 xdistilbert_customer19.tar.gz\r\n",
      "2023-02-01 04:16:58   46621941 xdistilbert_customer2.tar.gz\r\n",
      "2023-02-01 04:17:17   46621941 xdistilbert_customer20.tar.gz\r\n",
      "2023-02-01 04:17:18   46621941 xdistilbert_customer21.tar.gz\r\n",
      "2023-02-01 04:17:19   46621941 xdistilbert_customer22.tar.gz\r\n",
      "2023-02-01 04:17:20   46621941 xdistilbert_customer23.tar.gz\r\n",
      "2023-02-01 04:17:21   46621941 xdistilbert_customer24.tar.gz\r\n",
      "2023-02-01 04:17:22   46621941 xdistilbert_customer25.tar.gz\r\n",
      "2023-02-01 04:17:23   46621941 xdistilbert_customer26.tar.gz\r\n",
      "2023-02-01 04:17:24   46621941 xdistilbert_customer27.tar.gz\r\n",
      "2023-02-01 04:17:25   46621941 xdistilbert_customer28.tar.gz\r\n",
      "2023-02-01 04:17:26   46621941 xdistilbert_customer29.tar.gz\r\n",
      "2023-02-01 04:16:59   46621941 xdistilbert_customer3.tar.gz\r\n",
      "2023-02-01 04:17:27   46621941 xdistilbert_customer30.tar.gz\r\n",
      "2023-02-01 04:17:28   46621941 xdistilbert_customer31.tar.gz\r\n",
      "2023-02-01 04:17:29   46621941 xdistilbert_customer32.tar.gz\r\n",
      "2023-02-01 04:17:30   46621941 xdistilbert_customer33.tar.gz\r\n",
      "2023-02-01 04:17:31   46621941 xdistilbert_customer34.tar.gz\r\n",
      "2023-02-01 04:17:32   46621941 xdistilbert_customer35.tar.gz\r\n",
      "2023-02-01 04:17:33   46621941 xdistilbert_customer36.tar.gz\r\n",
      "2023-02-01 04:17:34   46621941 xdistilbert_customer37.tar.gz\r\n",
      "2023-02-01 04:17:35   46621941 xdistilbert_customer38.tar.gz\r\n",
      "2023-02-01 04:17:36   46621941 xdistilbert_customer39.tar.gz\r\n",
      "2023-02-01 04:17:00   46621941 xdistilbert_customer4.tar.gz\r\n",
      "2023-02-01 04:17:37   46621941 xdistilbert_customer40.tar.gz\r\n",
      "2023-02-01 04:17:39   46621941 xdistilbert_customer41.tar.gz\r\n",
      "2023-02-01 04:17:40   46621941 xdistilbert_customer42.tar.gz\r\n",
      "2023-02-01 04:17:41   46621941 xdistilbert_customer43.tar.gz\r\n",
      "2023-02-01 04:17:42   46621941 xdistilbert_customer44.tar.gz\r\n",
      "2023-02-01 04:17:42   46621941 xdistilbert_customer45.tar.gz\r\n",
      "2023-02-01 04:17:44   46621941 xdistilbert_customer46.tar.gz\r\n",
      "2023-02-01 04:17:45   46621941 xdistilbert_customer47.tar.gz\r\n",
      "2023-02-01 04:17:46   46621941 xdistilbert_customer48.tar.gz\r\n",
      "2023-02-01 04:17:47   46621941 xdistilbert_customer49.tar.gz\r\n",
      "2023-02-01 04:17:01   46621941 xdistilbert_customer5.tar.gz\r\n",
      "2023-02-01 04:17:48   46621941 xdistilbert_customer50.tar.gz\r\n",
      "2023-02-01 04:17:49   46621941 xdistilbert_customer51.tar.gz\r\n",
      "2023-02-01 04:17:50   46621941 xdistilbert_customer52.tar.gz\r\n",
      "2023-02-01 04:17:51   46621941 xdistilbert_customer53.tar.gz\r\n",
      "2023-02-01 04:17:52   46621941 xdistilbert_customer54.tar.gz\r\n",
      "2023-02-01 04:17:54   46621941 xdistilbert_customer55.tar.gz\r\n",
      "2023-02-01 04:17:55   46621941 xdistilbert_customer56.tar.gz\r\n",
      "2023-02-01 04:17:56   46621941 xdistilbert_customer57.tar.gz\r\n",
      "2023-02-01 04:17:57   46621941 xdistilbert_customer58.tar.gz\r\n",
      "2023-02-01 04:17:58   46621941 xdistilbert_customer59.tar.gz\r\n",
      "2023-02-01 04:17:02   46621941 xdistilbert_customer6.tar.gz\r\n",
      "2023-02-01 04:17:59   46621941 xdistilbert_customer60.tar.gz\r\n",
      "2023-02-01 04:18:00   46621941 xdistilbert_customer61.tar.gz\r\n",
      "2023-02-01 04:18:01   46621941 xdistilbert_customer62.tar.gz\r\n",
      "2023-02-01 04:18:02   46621941 xdistilbert_customer63.tar.gz\r\n",
      "2023-02-01 04:18:03   46621941 xdistilbert_customer64.tar.gz\r\n",
      "2023-02-01 04:18:04   46621941 xdistilbert_customer65.tar.gz\r\n",
      "2023-02-01 04:18:05   46621941 xdistilbert_customer66.tar.gz\r\n",
      "2023-02-01 04:18:06   46621941 xdistilbert_customer67.tar.gz\r\n",
      "2023-02-01 04:18:07   46621941 xdistilbert_customer68.tar.gz\r\n",
      "2023-02-01 04:18:08   46621941 xdistilbert_customer69.tar.gz\r\n",
      "2023-02-01 04:17:03   46621941 xdistilbert_customer7.tar.gz\r\n",
      "2023-02-01 04:18:09   46621941 xdistilbert_customer70.tar.gz\r\n",
      "2023-02-01 04:18:10   46621941 xdistilbert_customer71.tar.gz\r\n",
      "2023-02-01 04:18:11   46621941 xdistilbert_customer72.tar.gz\r\n",
      "2023-02-01 04:18:12   46621941 xdistilbert_customer73.tar.gz\r\n",
      "2023-02-01 04:18:13   46621941 xdistilbert_customer74.tar.gz\r\n",
      "2023-02-01 04:18:14   46621941 xdistilbert_customer75.tar.gz\r\n",
      "2023-02-01 04:18:15   46621941 xdistilbert_customer76.tar.gz\r\n",
      "2023-02-01 04:18:16   46621941 xdistilbert_customer77.tar.gz\r\n",
      "2023-02-01 04:18:17   46621941 xdistilbert_customer78.tar.gz\r\n",
      "2023-02-01 04:18:18   46621941 xdistilbert_customer79.tar.gz\r\n",
      "2023-02-01 04:17:04   46621941 xdistilbert_customer8.tar.gz\r\n",
      "2023-02-01 04:18:19   46621941 xdistilbert_customer80.tar.gz\r\n",
      "2023-02-01 04:18:20   46621941 xdistilbert_customer81.tar.gz\r\n",
      "2023-02-01 04:18:21   46621941 xdistilbert_customer82.tar.gz\r\n",
      "2023-02-01 04:18:22   46621941 xdistilbert_customer83.tar.gz\r\n",
      "2023-02-01 04:18:23   46621941 xdistilbert_customer84.tar.gz\r\n",
      "2023-02-01 04:18:24   46621941 xdistilbert_customer85.tar.gz\r\n",
      "2023-02-01 04:18:25   46621941 xdistilbert_customer86.tar.gz\r\n",
      "2023-02-01 04:18:26   46621941 xdistilbert_customer87.tar.gz\r\n",
      "2023-02-01 04:18:28   46621941 xdistilbert_customer88.tar.gz\r\n",
      "2023-02-01 04:18:29   46621941 xdistilbert_customer89.tar.gz\r\n",
      "2023-02-01 04:17:05   46621941 xdistilbert_customer9.tar.gz\r\n",
      "2023-02-01 04:18:30   46621941 xdistilbert_customer90.tar.gz\r\n",
      "2023-02-01 04:18:31   46621941 xdistilbert_customer91.tar.gz\r\n",
      "2023-02-01 04:18:32   46621941 xdistilbert_customer92.tar.gz\r\n",
      "2023-02-01 04:18:33   46621941 xdistilbert_customer93.tar.gz\r\n",
      "2023-02-01 04:18:34   46621941 xdistilbert_customer94.tar.gz\r\n",
      "2023-02-01 04:18:35   46621941 xdistilbert_customer95.tar.gz\r\n",
      "2023-02-01 04:18:36   46621941 xdistilbert_customer96.tar.gz\r\n",
      "2023-02-01 04:18:37   46621941 xdistilbert_customer97.tar.gz\r\n",
      "2023-02-01 04:18:38   46621941 xdistilbert_customer98.tar.gz\r\n",
      "2023-02-01 04:18:39   46621941 xdistilbert_customer99.tar.gz\r\n",
      "2023-02-01 04:05:53   46621941 xdistilbert_pt.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $model_data_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef26cf-3ca9-4702-88eb-963e794b69f4",
   "metadata": {},
   "source": [
    "# Dynamic Model Unloading Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(text, model_name, show_latency=False):\n",
    "    print(f\"Using model {model_name} to predict\")\n",
    "    \n",
    "    request_body, header_length = get_tokenized_text_binary_pt(text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel=model_name)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    prediction = read_response(response, output_name=\"OUTPUT__0\")\n",
    "    \n",
    "    if show_latency:\n",
    "        print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")\n",
    "    else:\n",
    "        print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c35875ae-e2b5-4626-a88e-6b77dc6c6f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer2.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer3.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer4.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer5.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer6.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer7.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer8.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer9.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer10.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer11.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer12.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer13.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer14.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer15.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer16.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer17.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer18.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer19.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer20.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer21.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer22.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer23.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer24.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer25.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer26.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer27.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer28.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer29.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer30.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer31.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer32.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer33.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer34.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer35.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer36.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer37.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer38.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer39.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer40.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer41.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer42.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer43.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer44.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer45.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer46.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer47.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer48.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer49.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer50.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer51.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer52.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer53.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer54.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer55.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer56.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer57.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer58.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer59.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer60.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer61.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer62.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer63.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer64.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer65.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer66.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer67.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer68.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer69.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer70.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer71.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer72.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer73.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer74.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer75.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer76.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer77.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer78.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer79.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer80.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer81.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer82.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer83.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer84.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer85.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer86.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer87.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer88.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer89.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer90.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer91.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer92.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer93.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer94.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer95.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer96.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer97.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer98.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer99.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer100.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer101.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer102.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer103.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer104.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer105.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer106.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer107.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer108.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer109.tar.gz to predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer110.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer111.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer112.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer113.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer114.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer115.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer116.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer117.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer118.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer119.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer120.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer121.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer122.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer123.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer124.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer125.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer126.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer127.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer128.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer129.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer130.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer131.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer132.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer133.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer134.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer135.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer136.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer137.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer138.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer139.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer140.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer141.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer142.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer143.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer144.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer145.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer146.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer147.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer148.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer149.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer150.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer151.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer152.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer153.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer154.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer155.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer156.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer157.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer158.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer159.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer160.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer161.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer162.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer163.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer164.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer165.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer166.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer167.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer168.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer169.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer170.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer171.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer172.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer173.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer174.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer175.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer176.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer177.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer178.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer179.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer180.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer181.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer182.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer183.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer184.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer185.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer186.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer187.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer188.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer189.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer190.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer191.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer192.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer193.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer194.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer195.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer196.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer197.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer198.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer199.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer200.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer201.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer202.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer203.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer204.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer205.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer206.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer207.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer208.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer209.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer210.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer211.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer212.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer213.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer214.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer215.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer216.tar.gz to predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer217.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer218.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer219.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer220.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer221.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer222.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer223.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer224.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer225.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer226.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer227.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer228.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer229.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer230.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer231.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer232.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer233.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer234.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer235.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer236.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer237.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer238.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer239.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer240.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer241.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer242.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer243.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer244.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer245.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer246.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer247.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer248.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer249.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer250.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer251.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer252.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer253.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer254.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer255.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer256.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer257.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer258.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer259.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer260.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer261.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer262.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer263.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer264.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer265.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer266.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer267.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer268.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer269.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer270.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer271.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer272.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer273.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer274.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer275.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer276.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer277.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer278.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer279.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer280.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer281.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer282.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer283.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer284.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer285.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer286.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer287.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer288.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer289.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer290.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer291.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer292.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer293.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer294.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer295.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer296.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer297.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer298.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer299.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 300):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(text=get_random_text(), model_name=customer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985760-04a4-4718-977e-05d12d1e39bf",
   "metadata": {},
   "source": [
    "[Show logs to show unloading]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b8331-282f-4d09-ba75-8893b382f743",
   "metadata": {},
   "source": [
    "[Show LoadedModelCount and GPUMemoryUtilization]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b006088-db0a-45e4-96a7-37e901d17e75",
   "metadata": {},
   "source": [
    "# Autoscaling Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f091ec-f438-41c0-b00a-71f50111b546",
   "metadata": {},
   "source": [
    "## Set up AutoScaling Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f2eb482-42ee-43f0-a7d9-2bb7328a5dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoscaling policy for GPU MME endpoint has been set up\n"
     ]
    }
   ],
   "source": [
    "auto_scaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' \n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity = 1,\n",
    "    MaxCapacity = 2\n",
    ")\n",
    "\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName='GPUMemUtil-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 75, \n",
    "        'CustomizedMetricSpecification':\n",
    "        {\n",
    "            'MetricName': 'GPUMemoryUtilization',\n",
    "            'Namespace': '/aws/sagemaker/Endpoints',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                {'Name': 'VariantName','Value': 'AllTraffic'}\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "            'Unit': 'Percent'\n",
    "        },\n",
    "        'ScaleInCooldown': 600,\n",
    "        'ScaleOutCooldown': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Autoscaling policy for GPU MME endpoint has been set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c8ad8-e116-454f-8ccc-cee513ef01ac",
   "metadata": {},
   "source": [
    "[Show CW alarm being triggered and Endpoint entering Updating state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dca6a-93cd-4045-aaa2-857e22a81311",
   "metadata": {},
   "source": [
    "### While Autoscaling the endpoint is still active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cef815d0-fc2f-4b0a-80ef-d9d925e78a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer200.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_model(text=get_random_text(), model_name=\"xdistilbert_customer200.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6014b7-f01f-4cfd-b7a1-43b7db2145d7",
   "metadata": {},
   "source": [
    "[Show Endpoint autoscaling to 2 instances]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5599ff-15b5-4bb3-9b7d-d22a87aab8f9",
   "metadata": {},
   "source": [
    "## Invoke Models Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86f33883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n",
      "prediction: ['LOVE'], took 935 ms\n",
      "\n",
      "Using model xdistilbert_customer2.tar.gz to predict\n",
      "prediction: ['LOVE'], took 774 ms\n",
      "\n",
      "Using model xdistilbert_customer3.tar.gz to predict\n",
      "prediction: ['LOVE'], took 874 ms\n",
      "\n",
      "Using model xdistilbert_customer4.tar.gz to predict\n",
      "prediction: ['LOVE'], took 774 ms\n",
      "\n",
      "Using model xdistilbert_customer5.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer6.tar.gz to predict\n",
      "prediction: ['LOVE'], took 874 ms\n",
      "\n",
      "Using model xdistilbert_customer7.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer8.tar.gz to predict\n",
      "prediction: ['LOVE'], took 773 ms\n",
      "\n",
      "Using model xdistilbert_customer9.tar.gz to predict\n",
      "prediction: ['LOVE'], took 973 ms\n",
      "\n",
      "Using model xdistilbert_customer10.tar.gz to predict\n",
      "prediction: ['LOVE'], took 674 ms\n",
      "\n",
      "Using model xdistilbert_customer11.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer12.tar.gz to predict\n",
      "prediction: ['LOVE'], took 747 ms\n",
      "\n",
      "Using model xdistilbert_customer13.tar.gz to predict\n",
      "prediction: ['LOVE'], took 677 ms\n",
      "\n",
      "Using model xdistilbert_customer14.tar.gz to predict\n",
      "prediction: ['LOVE'], took 774 ms\n",
      "\n",
      "Using model xdistilbert_customer15.tar.gz to predict\n",
      "prediction: ['LOVE'], took 774 ms\n",
      "\n",
      "Using model xdistilbert_customer16.tar.gz to predict\n",
      "prediction: ['LOVE'], took 874 ms\n",
      "\n",
      "Using model xdistilbert_customer17.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer18.tar.gz to predict\n",
      "prediction: ['LOVE'], took 821 ms\n",
      "\n",
      "Using model xdistilbert_customer19.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer20.tar.gz to predict\n",
      "prediction: ['LOVE'], took 723 ms\n",
      "\n",
      "Using model xdistilbert_customer21.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer22.tar.gz to predict\n",
      "prediction: ['LOVE'], took 846 ms\n",
      "\n",
      "Using model xdistilbert_customer23.tar.gz to predict\n",
      "prediction: ['LOVE'], took 851 ms\n",
      "\n",
      "Using model xdistilbert_customer24.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer25.tar.gz to predict\n",
      "prediction: ['LOVE'], took 750 ms\n",
      "\n",
      "Using model xdistilbert_customer26.tar.gz to predict\n",
      "prediction: ['LOVE'], took 871 ms\n",
      "\n",
      "Using model xdistilbert_customer27.tar.gz to predict\n",
      "prediction: ['LOVE'], took 701 ms\n",
      "\n",
      "Using model xdistilbert_customer28.tar.gz to predict\n",
      "prediction: ['LOVE'], took 822 ms\n",
      "\n",
      "Using model xdistilbert_customer29.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer30.tar.gz to predict\n",
      "prediction: ['LOVE'], took 722 ms\n",
      "\n",
      "Using model xdistilbert_customer31.tar.gz to predict\n",
      "prediction: ['LOVE'], took 774 ms\n",
      "\n",
      "Using model xdistilbert_customer32.tar.gz to predict\n",
      "prediction: ['LOVE'], took 823 ms\n",
      "\n",
      "Using model xdistilbert_customer33.tar.gz to predict\n",
      "prediction: ['LOVE'], took 875 ms\n",
      "\n",
      "Using model xdistilbert_customer34.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer35.tar.gz to predict\n",
      "prediction: ['LOVE'], took 725 ms\n",
      "\n",
      "Using model xdistilbert_customer36.tar.gz to predict\n",
      "prediction: ['LOVE'], took 925 ms\n",
      "\n",
      "Using model xdistilbert_customer37.tar.gz to predict\n",
      "prediction: ['LOVE'], took 798 ms\n",
      "\n",
      "Using model xdistilbert_customer38.tar.gz to predict\n",
      "prediction: ['LOVE'], took 899 ms\n",
      "\n",
      "Using model xdistilbert_customer39.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer40.tar.gz to predict\n",
      "prediction: ['LOVE'], took 797 ms\n",
      "\n",
      "Using model xdistilbert_customer41.tar.gz to predict\n",
      "prediction: ['LOVE'], took 825 ms\n",
      "\n",
      "Using model xdistilbert_customer42.tar.gz to predict\n",
      "prediction: ['LOVE'], took 723 ms\n",
      "\n",
      "Using model xdistilbert_customer43.tar.gz to predict\n",
      "prediction: ['LOVE'], took 925 ms\n",
      "\n",
      "Using model xdistilbert_customer44.tar.gz to predict\n",
      "prediction: ['LOVE'], took 895 ms\n",
      "\n",
      "Using model xdistilbert_customer45.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer46.tar.gz to predict\n",
      "prediction: ['LOVE'], took 749 ms\n",
      "\n",
      "Using model xdistilbert_customer47.tar.gz to predict\n",
      "prediction: ['LOVE'], took 775 ms\n",
      "\n",
      "Using model xdistilbert_customer48.tar.gz to predict\n",
      "prediction: ['LOVE'], took 724 ms\n",
      "\n",
      "Using model xdistilbert_customer49.tar.gz to predict\n",
      "prediction: ['LOVE'], took 699 ms\n",
      "\n",
      "Using model xdistilbert_customer50.tar.gz to predict\n",
      "prediction: ['LOVE'], took 871 ms\n",
      "\n",
      "Using model xdistilbert_customer51.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer52.tar.gz to predict\n",
      "prediction: ['LOVE'], took 847 ms\n",
      "\n",
      "Using model xdistilbert_customer53.tar.gz to predict\n",
      "prediction: ['LOVE'], took 751 ms\n",
      "\n",
      "Using model xdistilbert_customer54.tar.gz to predict\n",
      "prediction: ['LOVE'], took 771 ms\n",
      "\n",
      "Using model xdistilbert_customer55.tar.gz to predict\n",
      "prediction: ['LOVE'], took 751 ms\n",
      "\n",
      "Using model xdistilbert_customer56.tar.gz to predict\n",
      "prediction: ['LOVE'], took 772 ms\n",
      "\n",
      "Using model xdistilbert_customer57.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer58.tar.gz to predict\n",
      "prediction: ['LOVE'], took 797 ms\n",
      "\n",
      "Using model xdistilbert_customer59.tar.gz to predict\n",
      "prediction: ['LOVE'], took 802 ms\n",
      "\n",
      "Using model xdistilbert_customer60.tar.gz to predict\n",
      "prediction: ['LOVE'], took 699 ms\n",
      "\n",
      "Using model xdistilbert_customer61.tar.gz to predict\n",
      "prediction: ['LOVE'], took 748 ms\n",
      "\n",
      "Using model xdistilbert_customer62.tar.gz to predict\n",
      "prediction: ['LOVE'], took 773 ms\n",
      "\n",
      "Using model xdistilbert_customer63.tar.gz to predict\n",
      "prediction: ['LOVE'], took 799 ms\n",
      "\n",
      "Using model xdistilbert_customer64.tar.gz to predict\n",
      "prediction: ['LOVE'], took 700 ms\n",
      "\n",
      "Using model xdistilbert_customer65.tar.gz to predict\n",
      "prediction: ['LOVE'], took 775 ms\n",
      "\n",
      "Using model xdistilbert_customer66.tar.gz to predict\n",
      "prediction: ['LOVE'], took 798 ms\n",
      "\n",
      "Using model xdistilbert_customer67.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer68.tar.gz to predict\n",
      "prediction: ['LOVE'], took 721 ms\n",
      "\n",
      "Using model xdistilbert_customer69.tar.gz to predict\n",
      "prediction: ['LOVE'], took 824 ms\n",
      "\n",
      "Using model xdistilbert_customer70.tar.gz to predict\n",
      "prediction: ['LOVE'], took 799 ms\n",
      "\n",
      "Using model xdistilbert_customer71.tar.gz to predict\n",
      "prediction: ['LOVE'], took 725 ms\n",
      "\n",
      "Using model xdistilbert_customer72.tar.gz to predict\n",
      "prediction: ['LOVE'], took 798 ms\n",
      "\n",
      "Using model xdistilbert_customer73.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer74.tar.gz to predict\n",
      "prediction: ['LOVE'], took 749 ms\n",
      "\n",
      "Using model xdistilbert_customer75.tar.gz to predict\n",
      "prediction: ['LOVE'], took 823 ms\n",
      "\n",
      "Using model xdistilbert_customer76.tar.gz to predict\n",
      "prediction: ['LOVE'], took 948 ms\n",
      "\n",
      "Using model xdistilbert_customer77.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer78.tar.gz to predict\n",
      "prediction: ['LOVE'], took 749 ms\n",
      "\n",
      "Using model xdistilbert_customer79.tar.gz to predict\n",
      "prediction: ['LOVE'], took 749 ms\n",
      "\n",
      "Using model xdistilbert_customer80.tar.gz to predict\n",
      "prediction: ['LOVE'], took 798 ms\n",
      "\n",
      "Using model xdistilbert_customer81.tar.gz to predict\n",
      "prediction: ['LOVE'], took 874 ms\n",
      "\n",
      "Using model xdistilbert_customer82.tar.gz to predict\n",
      "prediction: ['LOVE'], took 747 ms\n",
      "\n",
      "Using model xdistilbert_customer83.tar.gz to predict\n",
      "prediction: ['LOVE'], took 828 ms\n",
      "\n",
      "Using model xdistilbert_customer84.tar.gz to predict\n",
      "prediction: ['LOVE'], took 871 ms\n",
      "\n",
      "Using model xdistilbert_customer85.tar.gz to predict\n",
      "prediction: ['LOVE'], took 1850 ms\n",
      "\n",
      "Using model xdistilbert_customer86.tar.gz to predict\n",
      "prediction: ['LOVE'], took 796 ms\n",
      "\n",
      "Using model xdistilbert_customer87.tar.gz to predict\n",
      "prediction: ['LOVE'], took 726 ms\n",
      "\n",
      "Using model xdistilbert_customer88.tar.gz to predict\n",
      "prediction: ['LOVE'], took 1822 ms\n",
      "\n",
      "Using model xdistilbert_customer89.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer90.tar.gz to predict\n",
      "prediction: ['LOVE'], took 823 ms\n",
      "\n",
      "Using model xdistilbert_customer91.tar.gz to predict\n",
      "prediction: ['LOVE'], took 775 ms\n",
      "\n",
      "Using model xdistilbert_customer92.tar.gz to predict\n",
      "prediction: ['LOVE'], took 771 ms\n",
      "\n",
      "Using model xdistilbert_customer93.tar.gz to predict\n",
      "prediction: ['LOVE'], took 726 ms\n",
      "\n",
      "Using model xdistilbert_customer94.tar.gz to predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: ['LOVE'], took 772 ms\n",
      "\n",
      "Using model xdistilbert_customer95.tar.gz to predict\n",
      "prediction: ['LOVE'], took 998 ms\n",
      "\n",
      "Using model xdistilbert_customer96.tar.gz to predict\n",
      "prediction: ['LOVE'], took 849 ms\n",
      "\n",
      "Using model xdistilbert_customer97.tar.gz to predict\n",
      "prediction: ['LOVE'], took 800 ms\n",
      "\n",
      "Using model xdistilbert_customer98.tar.gz to predict\n",
      "prediction: ['LOVE'], took 801 ms\n",
      "\n",
      "Using model xdistilbert_customer99.tar.gz to predict\n",
      "prediction: ['LOVE'], took 747 ms\n",
      "\n",
      "Using model xdistilbert_customer100.tar.gz to predict\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Failed to download model data(bucket: sagemaker-us-west-2-354625738399, key: nlp-mme-gpu/xdistilbert_customer100.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8809/2011698483.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcustomer_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"xdistilbert_customer{i}.tar.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_latency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8809/605355003.py\u001b[0m in \u001b[0;36mpredict_model\u001b[0;34m(text, model_name, show_latency)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                   \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/vnd.sagemaker-triton.binary+json;json-header-size={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                   \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Failed to download model data(bucket: sagemaker-us-west-2-354625738399, key: nlp-mme-gpu/xdistilbert_customer100.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 300):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(sample_text, customer_model_name, show_latency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62d92ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n",
      "prediction: ['LOVE'], took 11 ms\n",
      "\n",
      "Using model xdistilbert_customer2.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer3.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer4.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer5.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer6.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer7.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer8.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer9.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer10.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer11.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer12.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer13.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer14.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer15.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer16.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer17.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer18.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer19.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer20.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer21.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer22.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer23.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer24.tar.gz to predict\n",
      "prediction: ['LOVE'], took 10 ms\n",
      "\n",
      "Using model xdistilbert_customer25.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer26.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer27.tar.gz to predict\n",
      "prediction: ['LOVE'], took 7 ms\n",
      "\n",
      "Using model xdistilbert_customer28.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer29.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer30.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer31.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer32.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer33.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer34.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer35.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer36.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer37.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer38.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer39.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer40.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer41.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer42.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer43.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer44.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer45.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer46.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer47.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer48.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer49.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer50.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer51.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer52.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer53.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer54.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer55.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer56.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer57.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer58.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer59.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer60.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer61.tar.gz to predict\n",
      "prediction: ['LOVE'], took 7 ms\n",
      "\n",
      "Using model xdistilbert_customer62.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer63.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer64.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer65.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer66.tar.gz to predict\n",
      "prediction: ['LOVE'], took 10 ms\n",
      "\n",
      "Using model xdistilbert_customer67.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer68.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer69.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer70.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer71.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer72.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer73.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer74.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer75.tar.gz to predict\n",
      "prediction: ['LOVE'], took 10 ms\n",
      "\n",
      "Using model xdistilbert_customer76.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer77.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer78.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer79.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer80.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer81.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer82.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer83.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer84.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer85.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer86.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer87.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer88.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer89.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer90.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer91.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer92.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer93.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer94.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer95.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer96.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer97.tar.gz to predict\n",
      "prediction: ['LOVE'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer98.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n",
      "Using model xdistilbert_customer99.tar.gz to predict\n",
      "prediction: ['LOVE'], took 8 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(sample_text, customer_model_name, show_latency=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbec88f-cbdb-4567-9048-50c0d9746fae",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0f756-6a47-4a8a-970a-32493ddb27f2",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1081fd3f-48c1-4de1-bbfc-4368ae8e8742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'b3c5d7dc-8ffd-4bc3-9fb3-16795c8476d8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b3c5d7dc-8ffd-4bc3-9fb3-16795c8476d8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 01 Feb 2023 04:05:12 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31407206-2d9e-4cc8-83a7-8d9e8e4748c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
