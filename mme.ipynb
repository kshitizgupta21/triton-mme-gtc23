{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc3d654",
   "metadata": {},
   "source": [
    "Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacc476",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483eb5-f1ee-43ac-8f8b-3bc2a02a1fc3",
   "metadata": {},
   "source": [
    "## Installs <a class=\"anchor\" id=\"installs-and-set-up\"></a>\n",
    "\n",
    "Install required to packages using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2956ae-e471-401e-9eb7-87a69df9c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip boto3 sagemaker awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad33f47-8226-4a87-91b2-75c64850168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92279c0c-cd98-4928-8425-3530697fb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "#variables\n",
    "prefix = \"aim401-mme-gpu\"\n",
    "model_name = \"efficientnet_b0\"\n",
    "pytorch_model_file_name = f\"{model_name}_pt_v0.tar.gz\"\n",
    "tensorrt_model_file_name = f\"{model_name}_trt_v0.tar.gz\"\n",
    "\n",
    "# imports\n",
    "import boto3\n",
    "#import json\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker import get_execution_role\n",
    "#import numpy as np\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "%store -r\n",
    "\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"ml.g4dn.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae20d7-4602-4ae4-b03c-eaac517d560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker Triton Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25758107-1d7a-4085-8a6a-908844f03ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_regions = [\"us-east-1\",\"us-east-2\",\"us-west-1\",\"us-west-2\"]\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in allowed_regions:\n",
    "    raise Exception(\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "     \"875666977404.dkr.ecr.{region}.{base}/sagemaker-tritonserver-ensemble:22.10-py3\".format(\n",
    "     region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dede4-e603-4e48-adb8-2d5a55b8f0d0",
   "metadata": {},
   "source": [
    "## Creating Model Artifacts <a class=\"anchor\" id=\"pytorch-efficientnet-model\"></a>\n",
    "\n",
    "This section presents overview of steps to prepare efficientnet_b0 pre-trained model to be deployed on SageMaker MME using Triton Inference server model configurations. \n",
    "\n",
    "<div class=\"alert alert-info\"><strong> Note </strong>\n",
    "We are demonstrating deployment with\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb7345-7bee-4ade-9213-1bd7082ca900",
   "metadata": {},
   "source": [
    "### Prepare PyTorch Model  <a class=\"anchor\" id=\"create-pytorch-model\"></a>\n",
    "\n",
    "- First, we load a pre-trained efficientnet_b0 model using torchvision models package. \n",
    "- We save the model as model.pt file in TorchScript optimized and serialized format. \n",
    "- TorchScript takes an example inputs to do a model forward pass, so pass an input of dimension 3X224X224.\n",
    "- [generate_model_pytorch.sh](./workspace/generate_model_pytorch.sh) file contains scripts to generate a PyTorch EfficientNet B0 model.\n",
    "\n",
    "Run the cell below and check out the [pt_exporter.py](./workspace/pt_exporter.py) file for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d67ee3-e301-4edb-a6d4-d0b1f35ad2b3",
   "metadata": {},
   "source": [
    "### Export model artifacts to S3 <a class=\"anchor\" id=\"export-to-s3\"></a>\n",
    "\n",
    "SageMaker expects the model artifacts in below format, it should also satisfy Triton container requirements such as model name, version, config.pbtxt files etc. `tar` the folder containing the model file as `model.tar.gz` and upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c069a-2fad-4e22-891a-cf0a05920327",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-trt/efficientnet_b0/1/\n",
    "!mv -f workspace/model.plan triton-serve-trt/efficientnet_b0/1/\n",
    "!tar -C triton-serve-trt/ -czf $tensorrt_model_file_name efficientnet_b0\n",
    "model_uri_trt = sagemaker_session.upload_data(path=tensorrt_model_file_name, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49ab86-fcb1-467b-96a9-4ce518914b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TensorRT Model S3 location: {model_uri_trt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8f41-08e5-4999-98e7-92ee2852940d",
   "metadata": {},
   "source": [
    "### Deploy Models with MME <a class=\"anchor\" id=\"deploy-models-with-mme\"></a>\n",
    "\n",
    "We will now deploy EfficientNet B0 and T5 model with different framework backends i.e. PyTorch, Python and TensorRT to SageMaker MME.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Note </strong>\n",
    "you can deploy 100s of models. The models can use same framework. They can also use different frameworks as shown in this note.\n",
    "</div>\n",
    "\n",
    "We will use AWS SDK for Python (Boto) APIs [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model), [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) and [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint) to create a mulit-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1f6b1-c210-484f-8bfb-4c05f84be0d3",
   "metadata": {},
   "source": [
    "### Define the serving container  <a class=\"anchor\" id=\"define-container-def\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c2773-10f6-4b18-bf8b-6a974ae2daf2",
   "metadata": {},
   "source": [
    " In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load  and serve predictions. Set `Mode` to `MultiModel` to indicates SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU, see MME [container images](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-support) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b00914-a237-4455-a08c-d79b3715cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri, \"ModelDataUrl\": model_data_url, \"Mode\": \"MultiModel\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9ec8f-8d4e-4450-8e31-987cec14796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a MME object <a class=\"anchor\" id=\"create-mme-model-obj\"></a>\n",
    "\n",
    "Using the SageMaker boto3 client, create the model using [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the create model API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077c860-1ef6-42f6-b10b-c77e075d2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8e07e-1410-471e-9c2c-10e0c23db00f",
   "metadata": {},
   "source": [
    "### Define configuration for the MME<a class=\"anchor\" id=\"config-mme\"></a>\n",
    "\n",
    "Create a multi-model endpoint configuration using [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) boto3 API. Specify an accelerated GPU computing instance in InstanceType (we will use the same instance type that we are using to host our SageMaker Notebook). We recommend configuring your endpoints with at least two instances with real-life use-cases. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55e160-8aa0-4ef7-84f5-cb975df99f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943aee-7692-4048-9a47-256d8e21c112",
   "metadata": {},
   "source": [
    "### Create MME  <a class=\"anchor\" id=\"create-mme\"></a>\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc49275-e1c2-46c3-a8be-9e193c271262",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404e50c-9f66-4b27-9b9a-714f474cc1d9",
   "metadata": {},
   "source": [
    "### Describe MME <a class=\"anchor\" id=\"describe-mme\"></a>\n",
    "\n",
    "Now, we check the status of the endpoint using `describe_endpoint`. This step will take about 5 mins to complete and you should see \"Status: InService\" message before you proceed to next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ac9f8-8ba9-4f37-a2cb-7dc6d428885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53191bdd-050c-412f-b771-15e3d49e2604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd99243-9f2e-4954-8ec8-d9b9c8301ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ff9a8e",
   "metadata": {},
   "source": [
    "## Perform Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I feel irritated and rejected without anyone doing anything or saying anything\",\n",
    "         \"I become overwhelmed and feel defeated.\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12681a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdistilbert_emotion_classification_client.request_inference(texts,\n",
    "                              model_name=\"xdistilbert_pt\",\n",
    "                              triton_url=\"localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081fd3f-48c1-4de1-bbfc-4368ae8e8742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
