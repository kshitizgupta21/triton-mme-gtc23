{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7210102-aba1-42f4-a5f7-90bac19ac6b3",
   "metadata": {},
   "source": [
    "# Deploying AI Models from Multiple Frameworks on same GPU using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd954e",
   "metadata": {},
   "source": [
    "Amazon SageMaker multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now, customers can deploy 1000s of deep learning models on GPUs behind one SageMaker endpoint. MME will run multiple models on a GPU, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve the best price performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308d276",
   "metadata": {},
   "source": [
    "In this section we show how MME on GPU allows you to deploy ML models from different frameworks like PyTorch, TensorRT, TensorFlow, ONNX, etc. In this example, we show the deployment of PyTorch and TensorRT DistilBERT models on same GPU using SageMaker MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a49a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> NOTE </strong>\n",
    "Set <strong> conda_python3 </strong> as kernel when prompted to set the kernel for this notebook. This notebook was tested with the <strong> conda_python3 </strong> kernel on an Amazon SageMaker notebook instance of type <strong> g5.xlarge </strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483eb5-f1ee-43ac-8f8b-3bc2a02a1fc3",
   "metadata": {},
   "source": [
    "### Installs <a class=\"anchor\" id=\"installs-and-set-up\"></a>\n",
    "\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2956ae-e471-401e-9eb7-87a69df9c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip boto3 sagemaker awscli tritonclient[http] transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b537a3-72b8-40d9-aed4-415efd008d09",
   "metadata": {},
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92279c0c-cd98-4928-8425-3530697fb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from utils import print_safe\n",
    "\n",
    "# sagemaker variables\n",
    "prefix = \"nlp-mme-gpu\"\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dede4-e603-4e48-adb8-2d5a55b8f0d0",
   "metadata": {},
   "source": [
    "## Creating Model Artifacts <a class=\"anchor\" id=\"pytorch-efficientnet-model\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a4d1d",
   "metadata": {},
   "source": [
    "This section presents overview of steps to prepare HuggingFace DistilBERT models to be deployed on SageMaker MME using Triton Inference server model configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb7345-7bee-4ade-9213-1bd7082ca900",
   "metadata": {},
   "source": [
    "### Prepare PyTorch Model  <a class=\"anchor\" id=\"create-pytorch-model\"></a>\n",
    "\n",
    "\n",
    "* We load pre-trained PyTorch [Huggingface DistilBERT model](https://huggingface.co/bergum/xtremedistil-emotion) that was finetuned on emotion classification task. \n",
    "* Export it to Torchscript serialized format\n",
    "\n",
    "To perform these steps, we will be using our [pt_exporter.py](./workspace/pt_exporter.py) script and running it within the [PyTorch NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./workspace/pt_exporter.py\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bergum/xtremedistil-emotion\", torchscript=True)\n",
    "\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "bs = 224\n",
    "seq_len = 128\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.ones(bs, seq_len, dtype=torch.int).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "\n",
    "traced_model = torch.jit.trace(model, dummy_inputs)\n",
    "torch.jit.save(traced_model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b290a97",
   "metadata": {},
   "source": [
    "Run the cell below to finish preparing the PyTorch DistilBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae16a-1bde-422b-9635-5b6e005e59c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff064e3",
   "metadata": {},
   "source": [
    "#### Setup PyTorch Model Repository\n",
    "\n",
    "Now that model artifact is ready we need to set up a model repository containing the model artifact we want to serve along with a [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) file i.e. `config.pbtxt`. This is the expected structure of the model repository:\n",
    "```\n",
    "xdistilbert_pt\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.pt\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e007176",
   "metadata": {},
   "source": [
    "#### PyTorch Model configuration <a class=\"anchor\" id=\"create-pytorch-model-config\"></a>\n",
    "\n",
    "Model configuration file `config.pbtxt` contains the following:  \n",
    "- `name`: xdistilbert_pt\n",
    "- `backend`: pytorch\n",
    "- `max_batch_size`: maximum batch size 224 that the model supports\n",
    "- `input` and `output` tensor shapes with the `data_type` \n",
    "\n",
    "Additionally, you can specify [instance_group](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#instance-groups) and [dynamic_batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batcher) properties to achieve high performance inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a21ac8-012c-4e6c-8d17-27838295f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_pt/1\n",
    "!cp workspace/model.pt model_repository/xdistilbert_pt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c395e-c52a-4a28-a18b-7baa746fc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repository/xdistilbert_pt/config.pbtxt\n",
    "name: \"xdistilbert_pt\"\n",
    "backend: \"pytorch\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "    {\n",
    "    name: \"INPUT__2\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87036cb6-f830-495a-b2cc-9651ec93fe06",
   "metadata": {},
   "source": [
    "### Prepare TensorRT Model <a class=\"anchor\" id=\"create-tensorrt-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca98ddb-d90b-4095-b310-825d09774954",
   "metadata": {},
   "source": [
    "- We load pre-trained xdistilbert PyTorch model from Huggingface\n",
    "- Convert to onnx representation using torch onnx exporter.\n",
    "- Use TensorRT `trtexec` bash command to create the TensorRT model plan\n",
    "\n",
    "To perform these steps, we will be running the [generate_model_trt.sh](./workspace/generate_model_trt.sh) script inside the [PyTorch NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./workspace/generate_model_trt.sh\n",
    "\n",
    "echo \"Installing Transformers...\"\n",
    "pip -q install transformers[onnx]\n",
    "\n",
    "echo \"Exporting model to ONNX...\"\n",
    "python -m transformers.onnx --model=bergum/xtremedistil-emotion \\\n",
    "                            --feature=sequence-classification /workspace/onnx/\n",
    "\n",
    "export CUDA_MODULE_LOADING=LAZY\n",
    "echo \"Converting ONNX Model to TensorRT FP16 Plan...\"\n",
    "trtexec --onnx=/workspace/onnx/model.onnx \\\n",
    "        --saveEngine=/workspace/model.plan \\\n",
    "        --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "        --optShapes=input_ids:16x128,attention_mask:16x128,token_type_ids:16x128 \\\n",
    "        --maxShapes=input_ids:224x128,attention_mask:224x128,token_type_ids:224x128 \\\n",
    "        --fp16 \\\n",
    "        --verbose \\\n",
    "        --memPoolSize=workspace:14000 | tee conversion_trt.txt\n",
    "\n",
    "echo \"Finished exporting all models...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a06fd",
   "metadata": {},
   "source": [
    "Execute the cell below to finish preparing the TensorRT DistilBERT model.\n",
    "\n",
    "**Note**:\n",
    "This TensorRT optimization step takes around 10 minutes to complete. While the step is running, feel free to check out the logs in the cell below and watch this [video](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41306/?start=349) to learn more about the different TensorRT optimizations like *Kernel AutoTuning*, *Layer Fusion* and *Reduced Mixed Precision*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefd65d-f654-44b6-9569-0a1d5597a7f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_trt.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a293bd",
   "metadata": {},
   "source": [
    "#### Setup TensorRT Model Repository\n",
    "\n",
    "Similar to PyTorch model, this is the expected structure of the TensorRT model repository:\n",
    "```\n",
    "xdistilbert_trt\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.plan\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dec73",
   "metadata": {},
   "source": [
    "#### TensorRT Model configuration <a class=\"anchor\" id=\"create-pytorch-model-config\"></a>\n",
    "\n",
    "Model configuration file `config.pbtxt` contains the following  \n",
    "- `name`: xdistilbert_trt\n",
    "- `backend`: tensorrt\n",
    "- `max_batch_size`: maximum batch size 224 that the model supports\n",
    "- `input` and `output` tensor shapes with the `data_types`\n",
    "\n",
    "Additionally, you can specify [instance_group](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#instance-groups) and [dynamic_batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batcher) properties to achieve high performance inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610b257-20d2-4f16-8f05-e482d42608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_trt/1/\n",
    "!cp workspace/model.plan model_repository/xdistilbert_trt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d9fdb-b6d0-40fd-bd29-96a53a6a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repository/xdistilbert_trt/config.pbtxt\n",
    "name: \"xdistilbert_trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"token_type_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d67ee3-e301-4edb-a6d4-d0b1f35ad2b3",
   "metadata": {},
   "source": [
    "## Export model artifacts to S3 <a class=\"anchor\" id=\"export-to-s3\"></a>\n",
    "\n",
    "Next, we will package our models as `*.tar.gz` files for uploading to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022a48f-5d52-4697-8b12-f00fc297f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model_file_name = \"xdistilbert_pt.tar.gz\"\n",
    "!tar -C model_repository -czf $pytorch_model_file_name xdistilbert_pt\n",
    "model_uri_pt = sagemaker_session.upload_data(path=pytorch_model_file_name, key_prefix=prefix)\n",
    "print_safe(f\"PyTorch Model S3 location: {model_uri_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c069a-2fad-4e22-891a-cf0a05920327",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorrt_model_file_name = \"xdistilbert_trt.tar.gz\"\n",
    "!tar -C model_repository -czf $tensorrt_model_file_name xdistilbert_trt\n",
    "model_uri_trt = sagemaker_session.upload_data(path=tensorrt_model_file_name, key_prefix=prefix)\n",
    "print_safe(f\"TensorRT Model S3 location: {model_uri_trt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8f41-08e5-4999-98e7-92ee2852940d",
   "metadata": {},
   "source": [
    "## Setup GPU Multi-Model Endpoint <a class=\"anchor\" id=\"deploy-models-with-mme\"></a>\n",
    "\n",
    "We will now setup Multi-Model Endpoint on GPU where we can deploy our DistilBERT PyTorch and TensorRT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379000a-3116-4ed4-851b-a041e4cfb3bf",
   "metadata": {},
   "source": [
    "### SageMaker Triton Container Image\n",
    "\n",
    "First we define the SageMaker Triton container image we will be using that supports deploying multi-model endpoints with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25758107-1d7a-4085-8a6a-908844f03ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc897e94",
   "metadata": {},
   "source": [
    "### Define the serving container  <a class=\"anchor\" id=\"define-container-def\"></a>\n",
    "\n",
    "Next, we define the serving container based on the Triton SageMaker image\n",
    "* In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load  and serve predictions. \n",
    "* Set `Mode` to `MultiModel` to indicate SageMaker should create the endpoint with MME specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b00914-a237-4455-a08c-d79b3715cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "container = {\"Image\": mme_triton_image_uri, \"ModelDataUrl\": model_data_url, \"Mode\": \"MultiModel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e28483-e6cf-4769-b6f3-4ccaa61471ec",
   "metadata": {},
   "source": [
    "### Create SageMaker model <a class=\"anchor\" id=\"create-mme-model-obj\"></a>\n",
    "\n",
    "We start off by creating a sagemaker model from the model files we uploaded to s3 in the previous step. We do this using the SageMaker boto3 client and [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the `create model` API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077c860-1ef6-42f6-b10b-c77e075d2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "sm_model_name = f\"{prefix}-model-{ts}\"\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print_safe(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8e07e-1410-471e-9c2c-10e0c23db00f",
   "metadata": {},
   "source": [
    "### Define configuration for the MME<a class=\"anchor\" id=\"config-mme\"></a>\n",
    "\n",
    "Next, we create a multi-model endpoint configuration using [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) boto3 API. Specify an accelerated GPU computing instance in InstanceType **(we will use the same instance type that we are using to host our SageMaker Notebook)**. We recommend configuring your endpoints with at least two instances with real-life use-cases. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55e160-8aa0-4ef7-84f5-cb975df99f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": ml.g5.xlarge,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print_safe(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943aee-7692-4048-9a47-256d8e21c112",
   "metadata": {},
   "source": [
    "### Create MME  <a class=\"anchor\" id=\"create-mme\"></a>\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc49275-e1c2-46c3-a8be-9e193c271262",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print_safe(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ac9f8-8ba9-4f37-a2cb-7dc6d428885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print_safe(\"\\nArn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afddbf7",
   "metadata": {},
   "source": [
    "## Helper functions to prepare Inference Payload\n",
    "\n",
    "The following methods transforms a sample texts we will be using for inference into the payload that can be sent for inference to the Triton server. These will be used by PyTorch and TensorRT DistilBERT NLP models.\n",
    "\n",
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference.\n",
    "\n",
    "**Note:** With the `binary+json` format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `application/vnd.sagemaker-triton.binary+json;json-header-size={}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae74d9-1058-4686-9f36-e297277d26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize_text, logits2prediction, get_random_text\n",
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "def _get_tokenized_text_binary(text, input_names, output_names):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_ids, attention_mask, token_type_ids = tokenize_text(tokenizer, text)\n",
    "    inputs.append(httpclient.InferInput(input_names[0], input_ids.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], attention_mask.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[2], token_type_ids.shape, \"INT32\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True)\n",
    "    inputs[2].set_data_from_numpy(token_type_ids.astype(np.int32), binary_data=True)\n",
    "    \n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "def get_tokenized_text_binary_pt(text):\n",
    "    return _get_tokenized_text_binary(text, [\"INPUT__0\", \"INPUT__1\", \"INPUT__2\"], [\"OUTPUT__0\"])\n",
    "\n",
    "def get_tokenized_text_binary_trt(text):\n",
    "    return _get_tokenized_text_binary(text, [\"input_ids\", \"attention_mask\", \"token_type_ids\"], [\"logits\"])\n",
    "\n",
    "def read_response(response, output_name):\n",
    "    # Parse json header size length from the response\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(response['Body'].read(), header_length=int(header_length_str))\n",
    "    logits = result.as_numpy(output_name)\n",
    "    prediction = logits2prediction(logits)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887d53d-d9b4-41bf-ab74-07a86cecd04c",
   "metadata": {},
   "source": [
    "## Invoking Models on Multi-Model Endpoint\n",
    "\n",
    "Once the endpoint is successfully created, we can send inference request to multi-model endpoint using `invoke_enpoint` API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below\n",
    "\n",
    "### Invoke TensorRT Model <a class=\"anchor\" id=\"invoke-tensorrt-model\"></a>\n",
    "\n",
    "Let's invoke the same model twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a84bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_trt(sample_text)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_trt.tar.gz')\n",
    "duration = time.time() - start_time\n",
    "\n",
    "output_name = 'logits'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_trt(sample_text)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_trt.tar.gz')\n",
    "duration = time.time() - start_time\n",
    "\n",
    "output_name = 'logits'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce77ad2",
   "metadata": {},
   "source": [
    "Notice the higher latencies on the first invocation of any given model. This is due to the time it takes SageMaker to download the model to the Endpoint instance and then load the model into the inference container. Subsequent invocations of the same model take advantage of the model already being loaded into the inference container and so are fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a05c-ae58-484c-9fed-4c9584795c7f",
   "metadata": {},
   "source": [
    "### Invoke PyTorch Model <a class=\"anchor\" id=\"invoke-pytorch-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab429b-5b9c-4edd-9ff7-5f974dd35924",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_pt(sample_text)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=f\"application/vnd.sagemaker-triton.binary+json;json-header-size={header_length}\",\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_pt.tar.gz')\n",
    "\n",
    "output_name = 'OUTPUT__0'\n",
    "prediction = read_response(response, output_name)\n",
    "print(f\"text: {sample_text}\\n\")\n",
    "print(f\"prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa3d1a-c0f2-4809-a7e2-71056556c864",
   "metadata": {},
   "source": [
    "# Deploying Hundreds of Models to GPUs using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f67b3-0eb6-4ead-8851-c89b2628cb1c",
   "metadata": {},
   "source": [
    "Let's say you are trying to deploy 300 customer-specific distilBERT models which are a mixture of frequently and infrequently accessed models which need GPU acceleration for good inference performance. These models might even be coming from different frameworks like PyTorch, TensorFlow, ONNX, TensorRT.\n",
    "\n",
    "Deploying these 300 models on GPU instances like `g5.xlarge` using dedicated Single-Model Endpoints would take ~300 instances.\n",
    "\n",
    "By deploying these models behind a Multi-Model endpoint on GPUs you can end up using ~100x less instances. Thus reducing costs by **100x**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedb827-b9e7-4b54-98f4-f3effa813bc5",
   "metadata": {},
   "source": [
    "## Dynamically adding models to an existing endpoint\n",
    "\n",
    "Itâ€™s easy to deploy a new model to an existing multi-model endpoint. With the endpoint already running, copy a new set of model artifacts to the same S3 location you set up earlier. Client applications are then free to request predictions from that target model, and Amazon SageMaker handles the rest. \n",
    "\n",
    "This step below will take around few minutes to complete as we are copying 300 files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd99243-9f2e-4954-8ec8-d9b9c8301ddb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "num_models = 300\n",
    "for i in range(num_models):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    model_copy = f\"{model_data_url}{customer_model_name}\"\n",
    "    !aws s3 cp $model_data_url$pytorch_model_file_name $model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccbc78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls $model_data_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724cccf",
   "metadata": {},
   "source": [
    "### Helper function to invoke models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(text, model_name, show_latency=False):\n",
    "    print(f\"Using model {model_name} to predict\")\n",
    "    \n",
    "    request_body, header_length = get_tokenized_text_binary_pt(text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel=model_name)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    prediction = read_response(response, output_name=\"OUTPUT__0\")\n",
    "    \n",
    "    if show_latency:\n",
    "        print(f\"prediction: {prediction}, took {int(duration * 1000)} ms\\n\")\n",
    "    else:\n",
    "        print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(text=get_random_text(), model_name=\"xdistilbert_customer1.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065544db",
   "metadata": {},
   "source": [
    "## Monitoring GPU MME using Amazon CloudWatch metrics\n",
    "To make price and performance tradeoffs, you will want to test multi-model endpoints with models and representative traffic from your own application. Amazon SageMaker provides metrics in CloudWatch for multi-model endpoints so you can determine the endpoint usage and the cache hit rate and optimize your endpoint.\n",
    "\n",
    "* **LoadedModelCount**: The number of models loaded in the containers of the multi-model endpoint\n",
    "* **GPUUtilization**: Precentage of GPU units that are used by the containers on an instance \n",
    "* **GPUMemoryUtilization**: Precentage of GPU memory used by the containers on an instance \n",
    "* **DiskUtilization**: Precentage of disk space used by the containers on an instance \n",
    "and others\n",
    "\n",
    "SageMaker MME also provides Model loading metrics such as:\n",
    "\n",
    "* **ModelLoadingWaitTime**: Time interval for model to be downloaded or loaded\n",
    "* **ModelUnloadingTime**: Time interval to unload model from container\n",
    "* **ModelDownloadingTime** Time to download the model from S3\n",
    "* **ModelCacheHit**: Number of invocations to model that are already loaded onto the container\n",
    "\n",
    "For more details refer [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef26cf-3ca9-4702-88eb-963e794b69f4",
   "metadata": {},
   "source": [
    "# Dynamic Model Unloading in MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089cd1f",
   "metadata": {},
   "source": [
    "In this section, we show dynamic loading and unloading of models based on the resource utilization on the GPU based compute instance. When the `GPUMemoryUtilization` is at the maximum and SageMaker cannot load more models to serve the requests, SageMaker unloads previously loaded models to free up GPU memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87edd6",
   "metadata": {},
   "source": [
    "In this case, 1 `g5.xlarge` instance's NVIDIA A10G GPU (24GB memory) can handle 270 models in memory and beyond that if requests for newer models are made MME starts unloading LRU models to make room for newer models which can be seen in the CloudWatch logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35875ae-e2b5-4626-a88e-6b77dc6c6f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(300):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(text=get_random_text(), model_name=customer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b006088-db0a-45e4-96a7-37e901d17e75",
   "metadata": {},
   "source": [
    "# Autoscaling in MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e4f89",
   "metadata": {},
   "source": [
    "Amazon SageMaker multi-model endpoints on GPU fully support automatic scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f091ec-f438-41c0-b00a-71f50111b546",
   "metadata": {},
   "source": [
    "## Set up AutoScaling Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2eb482-42ee-43f0-a7d9-2bb7328a5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_scaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' \n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity = 1,\n",
    "    MaxCapacity = 2\n",
    ")\n",
    "\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName='GPUMemUtil-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 75, \n",
    "        'CustomizedMetricSpecification':\n",
    "        {\n",
    "            'MetricName': 'GPUMemoryUtilization',\n",
    "            'Namespace': '/aws/sagemaker/Endpoints',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                {'Name': 'VariantName','Value': 'AllTraffic'}\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "            'Unit': 'Percent'\n",
    "        },\n",
    "        'ScaleInCooldown': 600,\n",
    "        'ScaleOutCooldown': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Autoscaling policy for GPU MME endpoint has been set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dca6a-93cd-4045-aaa2-857e22a81311",
   "metadata": {},
   "source": [
    "### During Autoscaling the endpoint is still active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef815d0-fc2f-4b0a-80ef-d9d925e78a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(text=get_random_text(), model_name=\"xdistilbert_customer200.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc675900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612c721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f0417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a33b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b62ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e5599ff-15b5-4bb3-9b7d-d22a87aab8f9",
   "metadata": {},
   "source": [
    "## Invoke All Models Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f33883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 300):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(sample_text, customer_model_name, show_latency=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41f68a",
   "metadata": {},
   "source": [
    "## Matching your endpoint configuration to your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858829eb",
   "metadata": {},
   "source": [
    "In some cases, you may opt to reduce costs by choosing an instance type that cannot hold all the targeted models in memory at the same time. Amazon SageMaker unloads models dynamically when it runs out of memory to make room for a newly-targeted model. For infrequently requested models, the dynamic load latency may still be acceptable given the resulting lower costs. In cases with more stringent latency needs, you may opt for larger instance types or more instances. Investing time up front to do use-case specific testing and analysis with your multi-model endpoint will help to best optimize cost while meeting the performance needs of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbec88f-cbdb-4567-9048-50c0d9746fae",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0f756-6a47-4a8a-970a-32493ddb27f2",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081fd3f-48c1-4de1-bbfc-4368ae8e8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
