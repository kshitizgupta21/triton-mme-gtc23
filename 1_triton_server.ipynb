{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae23a6d",
   "metadata": {},
   "source": [
    "Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52030df1",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fa1dc",
   "metadata": {},
   "source": [
    "# Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85259973-9af7-4b82-8561-5adbba7a4d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/triton.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb8745",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook along with accompanying triton_client.ipynb notebook demonstrate deploying an Object Detection Model with the Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b378b8c-0997-462f-802d-3aba71ccb955",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac7a4a5-0a30-4a12-bf31-70ddd7b04c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 24em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# enlarge scrollable output size\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 24em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1d509",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup <a class=\"anchor\" id=\"Setup\"></a>\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511ce328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  2 00:08:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    38W / 300W |    282MiB / 32505MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5752a80d-7771-4a95-b059-e21af900ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA Tesla V100-DGXS-32GB (UUID: GPU-0f04e528-7346-af7f-c217-804407855015)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee600f",
   "metadata": {},
   "source": [
    "## Start the Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f035970",
   "metadata": {},
   "source": [
    "Let's go ahead and start the triton server in [polling mode](https://github.com/triton-inference-server/server/blob/8d02c9e8f1075baf525614120514ec2e12d3e842/docs/model_management.md#model-control-mode-poll) where changes to the model repository will be detected and Triton will attempt to load and unload models as necessary based on those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986042d7-b7e1-4023-8bae-5d97a2a8f3be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0602 00:08:30.381931 159 libtorch.cc:1381] TRITONBACKEND_Initialize: pytorch\n",
      "I0602 00:08:30.382019 159 libtorch.cc:1391] Triton TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.382030 159 libtorch.cc:1397] 'pytorch' TRITONBACKEND API version: 1.9\n",
      "2022-06-02 00:08:30.578741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "I0602 00:08:30.622366 159 tensorflow.cc:2181] TRITONBACKEND_Initialize: tensorflow\n",
      "I0602 00:08:30.622403 159 tensorflow.cc:2191] Triton TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.622416 159 tensorflow.cc:2197] 'tensorflow' TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.622424 159 tensorflow.cc:2221] backend configuration:\n",
      "{}\n",
      "I0602 00:08:30.624516 159 onnxruntime.cc:2400] TRITONBACKEND_Initialize: onnxruntime\n",
      "I0602 00:08:30.624536 159 onnxruntime.cc:2410] Triton TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.624545 159 onnxruntime.cc:2416] 'onnxruntime' TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.624553 159 onnxruntime.cc:2446] backend configuration:\n",
      "{}\n",
      "I0602 00:08:30.645901 159 openvino.cc:1207] TRITONBACKEND_Initialize: openvino\n",
      "I0602 00:08:30.645920 159 openvino.cc:1217] Triton TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:30.645930 159 openvino.cc:1223] 'openvino' TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:32.521433 159 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fa91e000000' with size 268435456\n",
      "I0602 00:08:32.523217 159 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0602 00:08:32.527355 159 model_repository_manager.cc:1077] loading: distilbert_trt:1\n",
      "I0602 00:08:32.627580 159 model_repository_manager.cc:1077] loading: distilbert_pt:1\n",
      "I0602 00:08:32.628304 159 tensorrt.cc:5294] TRITONBACKEND_Initialize: tensorrt\n",
      "I0602 00:08:32.628337 159 tensorrt.cc:5304] Triton TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:32.628353 159 tensorrt.cc:5310] 'tensorrt' TRITONBACKEND API version: 1.9\n",
      "I0602 00:08:32.628447 159 tensorrt.cc:5353] backend configuration:\n",
      "{}\n",
      "I0602 00:08:32.628484 159 tensorrt.cc:5405] TRITONBACKEND_ModelInitialize: distilbert_trt (version 1)\n",
      "I0602 00:08:32.630382 159 tensorrt.cc:5454] TRITONBACKEND_ModelInstanceInitialize: distilbert_trt_0 (GPU device 0)\n",
      "I0602 00:08:33.068232 159 logging.cc:49] [MemUsageChange] Init CUDA: CPU +252, GPU +0, now: CPU 1412, GPU 1290 (MiB)\n",
      "I0602 00:08:33.432858 159 logging.cc:49] Loaded engine size: 261 MiB\n",
      "I0602 00:08:34.135231 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +385, GPU +180, now: CPU 2453, GPU 1600 (MiB)\n",
      "I0602 00:08:34.325447 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +117, GPU +52, now: CPU 2570, GPU 1652 (MiB)\n",
      "I0602 00:08:34.327128 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +128, now: CPU 0, GPU 128 (MiB)\n",
      "I0602 00:08:34.377887 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2047, GPU 1644 (MiB)\n",
      "I0602 00:08:34.381197 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2047, GPU 1652 (MiB)\n",
      "I0602 00:08:35.097915 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +547, now: CPU 0, GPU 675 (MiB)\n",
      "I0602 00:08:35.098137 159 tensorrt.cc:1411] Created instance distilbert_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0602 00:08:35.098191 159 libtorch.cc:1430] TRITONBACKEND_ModelInitialize: distilbert_pt (version 1)\n",
      "I0602 00:08:35.098327 159 model_repository_manager.cc:1231] successfully loaded 'distilbert_trt' version 1\n",
      "I0602 00:08:35.099502 159 libtorch.cc:293] Optimized execution is enabled for model instance 'distilbert_pt'\n",
      "I0602 00:08:35.099539 159 libtorch.cc:311] Inference Mode is disabled for model instance 'distilbert_pt'\n",
      "I0602 00:08:35.099557 159 libtorch.cc:406] NvFuser is not specified for model instance 'distilbert_pt'\n",
      "I0602 00:08:35.100295 159 libtorch.cc:1474] TRITONBACKEND_ModelInstanceInitialize: distilbert_pt_0 (GPU device 0)\n",
      "I0602 00:08:35.809506 159 model_repository_manager.cc:1231] successfully loaded 'distilbert_pt' version 1\n",
      "I0602 00:08:35.809636 159 server.cc:549] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0602 00:08:35.809798 159 server.cc:576] \n",
      "+-------------+------------------------------------------------------+--------+\n",
      "| Backend     | Path                                                 | Config |\n",
      "+-------------+------------------------------------------------------+--------+\n",
      "| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch | {}     |\n",
      "|             | .so                                                  |        |\n",
      "| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_ten | {}     |\n",
      "|             | sorflow1.so                                          |        |\n",
      "| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onn | {}     |\n",
      "|             | xruntime.so                                          |        |\n",
      "| openvino    | /opt/tritonserver/backends/openvino_2021_4/libtriton | {}     |\n",
      "|             | _openvino_2021_4.so                                  |        |\n",
      "| tensorrt    | /opt/tritonserver/backends/tensorrt/libtriton_tensor | {}     |\n",
      "|             | rt.so                                                |        |\n",
      "+-------------+------------------------------------------------------+--------+\n",
      "\n",
      "I0602 00:08:35.809910 159 server.cc:619] \n",
      "+----------------+---------+--------+\n",
      "| Model          | Version | Status |\n",
      "+----------------+---------+--------+\n",
      "| distilbert_pt  | 1       | READY  |\n",
      "| distilbert_trt | 1       | READY  |\n",
      "+----------------+---------+--------+\n",
      "\n",
      "I0602 00:08:35.864799 159 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA Tesla V100-DGXS-32GB\n",
      "I0602 00:08:35.865323 159 tritonserver.cc:2123] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.21.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /workspace/model_repository              |\n",
      "| model_control_mode               | MODE_POLL                                |\n",
      "| strict_model_config              | 1                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0602 00:08:35.866750 159 grpc_server.cc:4544] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0602 00:08:35.867057 159 http_server.cc:3242] Started HTTPService at 0.0.0.0:8000\n",
      "I0602 00:08:35.908217 159 http_server.cc:180] Started Metrics Service at 0.0.0.0:8002\n",
      "I0602 00:10:20.916104 159 model_repository_manager.cc:1109] unloading: distilbert_pt:1\n",
      "I0602 00:10:20.945675 159 libtorch.cc:1508] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0602 00:10:20.954978 159 libtorch.cc:1453] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0602 00:10:20.955370 159 model_repository_manager.cc:1214] successfully unloaded 'distilbert_pt' version 1\n",
      "I0602 00:13:05.921015 159 model_repository_manager.cc:1077] loading: distilbert_pt:1\n",
      "I0602 00:13:06.021320 159 libtorch.cc:1430] TRITONBACKEND_ModelInitialize: distilbert_pt (version 1)\n",
      "I0602 00:13:06.022204 159 libtorch.cc:293] Optimized execution is enabled for model instance 'distilbert_pt'\n",
      "I0602 00:13:06.022238 159 libtorch.cc:311] Inference Mode is disabled for model instance 'distilbert_pt'\n",
      "I0602 00:13:06.022260 159 libtorch.cc:406] NvFuser is not specified for model instance 'distilbert_pt'\n",
      "I0602 00:13:06.027203 159 libtorch.cc:1474] TRITONBACKEND_ModelInstanceInitialize: distilbert_pt_0 (GPU device 0)\n",
      "I0602 00:13:06.631528 159 model_repository_manager.cc:1231] successfully loaded 'distilbert_pt' version 1\n",
      "W0602 00:13:51.635716 159 model_repository_manager.cc:202] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0602 00:13:51.635786 159 model_repository_manager.cc:1077] loading: distilbert_trt:1\n",
      "I0602 00:13:51.736234 159 tensorrt.cc:5405] TRITONBACKEND_ModelInitialize: distilbert_trt (version 1)\n",
      "I0602 00:13:51.741896 159 tensorrt.cc:5454] TRITONBACKEND_ModelInstanceInitialize: distilbert_trt_0 (GPU device 0)\n",
      "I0602 00:13:51.744431 159 logging.cc:49] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2132, GPU 2510 (MiB)\n",
      "I0602 00:13:52.148359 159 logging.cc:49] Loaded engine size: 261 MiB\n",
      "I0602 00:13:52.253881 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2787, GPU 2648 (MiB)\n",
      "I0602 00:13:52.257560 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 2788, GPU 2658 (MiB)\n",
      "I0602 00:13:52.258161 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +128, now: CPU 0, GPU 803 (MiB)\n",
      "I0602 00:13:52.292121 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2264, GPU 2650 (MiB)\n",
      "I0602 00:13:52.293374 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2265, GPU 2658 (MiB)\n",
      "I0602 00:13:53.018081 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +547, now: CPU 0, GPU 1350 (MiB)\n",
      "I0602 00:13:53.018296 159 tensorrt.cc:1411] Created instance distilbert_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0602 00:13:53.018474 159 model_repository_manager.cc:1231] successfully loaded 'distilbert_trt' version 1\n",
      "I0602 00:13:53.018492 159 model_repository_manager.cc:1109] unloading: distilbert_trt:1\n",
      "I0602 00:13:53.018769 159 tensorrt.cc:5492] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0602 00:13:53.052445 159 tensorrt.cc:5431] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0602 00:13:53.052490 159 model_repository_manager.cc:1214] successfully unloaded 'distilbert_trt' version 1\n",
      "I0602 00:15:08.020676 159 model_repository_manager.cc:1109] unloading: distilbert_pt:1\n",
      "I0602 00:15:08.021003 159 libtorch.cc:1508] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0602 00:15:08.033246 159 libtorch.cc:1453] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0602 00:15:08.035957 159 model_repository_manager.cc:1214] successfully unloaded 'distilbert_pt' version 1\n",
      "W0602 00:15:23.023931 159 model_repository_manager.cc:202] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0602 00:15:23.023997 159 model_repository_manager.cc:1077] loading: distilbert_trt:1\n",
      "I0602 00:15:23.124271 159 tensorrt.cc:5405] TRITONBACKEND_ModelInitialize: distilbert_trt (version 1)\n",
      "I0602 00:15:23.127951 159 tensorrt.cc:5454] TRITONBACKEND_ModelInstanceInitialize: distilbert_trt_0 (GPU device 0)\n",
      "I0602 00:15:23.130525 159 logging.cc:49] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2126, GPU 2220 (MiB)\n",
      "I0602 00:15:23.529147 159 logging.cc:49] Loaded engine size: 261 MiB\n",
      "I0602 00:15:23.641519 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2781, GPU 2358 (MiB)\n",
      "I0602 00:15:23.643676 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2781, GPU 2366 (MiB)\n",
      "I0602 00:15:23.644558 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +128, now: CPU 0, GPU 803 (MiB)\n",
      "I0602 00:15:23.681855 159 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2258, GPU 2358 (MiB)\n",
      "I0602 00:15:23.683700 159 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2258, GPU 2366 (MiB)\n",
      "I0602 00:15:24.409013 159 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +547, now: CPU 0, GPU 1350 (MiB)\n",
      "I0602 00:15:24.409213 159 tensorrt.cc:1411] Created instance distilbert_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0602 00:15:24.409432 159 model_repository_manager.cc:1231] successfully loaded 'distilbert_trt' version 1\n",
      "I0602 00:15:24.409449 159 model_repository_manager.cc:1109] unloading: distilbert_trt:1\n",
      "I0602 00:15:24.409625 159 tensorrt.cc:5492] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0602 00:15:24.442831 159 tensorrt.cc:5431] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0602 00:15:24.442878 159 model_repository_manager.cc:1214] successfully unloaded 'distilbert_trt' version 1\n"
     ]
    }
   ],
   "source": [
    "!tritonserver  --model-repository=/workspace/model_repository --model-control-mode=POLL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3ada3",
   "metadata": {},
   "source": [
    "The above command should load the model from the model directory and print the log `successfully loaded 'distillbert_pt' version 1` along with `distillbert_trt' version 1`\n",
    "\n",
    "```\n",
    "+-----------------+---------+--------+\n",
    "| Model           | Version | Status |\n",
    "+-----------------+---------+--------+\n",
    "| distillbert_pt  | 1       | READY  |\n",
    "+-----------------+---------+--------+\n",
    "| distillbert_trt | 1       | READY  |\n",
    "+-----------------+---------+--------+\n",
    "```\n",
    "\n",
    "Triton server listens on the following endpoints:\n",
    "\n",
    "```\n",
    "Port 8000    -> HTTP Service\n",
    "Port 8001    -> GRPC Service\n",
    "Port 8002    -> Metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1b081-3a54-437b-b860-cb5ab560fc9e",
   "metadata": {},
   "source": [
    "## Open The Client Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfed80-aacd-4c2d-9a48-5330f42a09c8",
   "metadata": {},
   "source": [
    "Now please open `triton_client.ipynb` and run through its cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
