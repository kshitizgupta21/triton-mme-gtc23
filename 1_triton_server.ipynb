{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae23a6d",
   "metadata": {},
   "source": [
    "Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52030df1",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fa1dc",
   "metadata": {},
   "source": [
    "# Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85259973-9af7-4b82-8561-5adbba7a4d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/triton.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb8745",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook along with accompanying triton_client.ipynb notebook demonstrate deploying an Object Detection Model with the Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b378b8c-0997-462f-802d-3aba71ccb955",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7a4a5-0a30-4a12-bf31-70ddd7b04c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enlarge scrollable output size\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 24em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1d509",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup <a class=\"anchor\" id=\"Setup\"></a>\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ce328",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752a80d-7771-4a95-b059-e21af900ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee600f",
   "metadata": {},
   "source": [
    "## Start the Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f035970",
   "metadata": {},
   "source": [
    "Let's go ahead and start the triton server in [polling mode](https://github.com/triton-inference-server/server/blob/8d02c9e8f1075baf525614120514ec2e12d3e842/docs/model_management.md#model-control-mode-poll) where changes to the model repository will be detected and Triton will attempt to load and unload models as necessary based on those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986042d7-b7e1-4023-8bae-5d97a2a8f3be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tritonserver  --model-repository=/workspace/model_repository --model-control-mode=POLL  --exit-on-error=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3ada3",
   "metadata": {},
   "source": [
    "The above command should load the model from the model directory and print the log `successfully loaded 'xdistillbert_pt' version 1` along with `xdistillbert_trt' version 1`\n",
    "\n",
    "```\n",
    "+-----------------+---------+--------+\n",
    "| Model           | Version | Status |\n",
    "+-----------------+---------+--------+\n",
    "| xdistillbert_pt  | 1       | READY  |\n",
    "+-----------------+---------+--------+\n",
    "| xdistillbert_trt | 1       | READY  |\n",
    "+-----------------+---------+--------+\n",
    "```\n",
    "\n",
    "Triton server listens on the following endpoints:\n",
    "\n",
    "```\n",
    "Port 8000    -> HTTP Service\n",
    "Port 8001    -> GRPC Service\n",
    "Port 8002    -> Metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58eb64-60e8-4669-bea9-467e28399b85",
   "metadata": {},
   "source": [
    "**You can safely ignore any messages related to `.ipynb_checkpoints`**. It's caused by Jupyter lab creating checkpoints for all the config files we edit inside the Triton model repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1b081-3a54-437b-b860-cb5ab560fc9e",
   "metadata": {},
   "source": [
    "## Open The Client Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfed80-aacd-4c2d-9a48-5330f42a09c8",
   "metadata": {},
   "source": [
    "Now please open `triton_client.ipynb` and run through its cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
