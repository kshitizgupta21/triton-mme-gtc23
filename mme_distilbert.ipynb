{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7210102-aba1-42f4-a5f7-90bac19ac6b3",
   "metadata": {},
   "source": [
    "# Deploying Models from Multiple Frameworks on GPU using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483eb5-f1ee-43ac-8f8b-3bc2a02a1fc3",
   "metadata": {},
   "source": [
    "## Installs <a class=\"anchor\" id=\"installs-and-set-up\"></a>\n",
    "\n",
    "Install required packages using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2956ae-e471-401e-9eb7-87a69df9c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip boto3 sagemaker awscli tritonclient[http] transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b537a3-72b8-40d9-aed4-415efd008d09",
   "metadata": {},
   "source": [
    "#### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92279c0c-cd98-4928-8425-3530697fb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#variables\n",
    "prefix = \"nlp-mme-gpu\"\n",
    "model_name = \"xdistilbert\"\n",
    "pytorch_model_file_name = f\"{model_name}_pt.tar.gz\"\n",
    "tensorrt_model_file_name = f\"{model_name}_trt.tar.gz\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"ml.g5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dede4-e603-4e48-adb8-2d5a55b8f0d0",
   "metadata": {},
   "source": [
    "## Creating Model Artifacts <a class=\"anchor\" id=\"pytorch-efficientnet-model\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb7345-7bee-4ade-9213-1bd7082ca900",
   "metadata": {},
   "source": [
    "### Prepare PyTorch Model  <a class=\"anchor\" id=\"create-pytorch-model\"></a>\n",
    "\n",
    "Run the cell below and check out the [pt_exporter.py](./workspace/pt_exporter.py) file for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae16a-1bde-422b-9635-5b6e005e59c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a21ac8-012c-4e6c-8d17-27838295f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c395e-c52a-4a28-a18b-7baa746fc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repository/xdistilbert_pt/config.pbtxt\n",
    "backend: \"pytorch\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "    {\n",
    "    name: \"INPUT__2\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87036cb6-f830-495a-b2cc-9651ec93fe06",
   "metadata": {},
   "source": [
    "### Prepare TensorRT Model <a class=\"anchor\" id=\"create-tensorrt-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca98ddb-d90b-4095-b310-825d09774954",
   "metadata": {},
   "source": [
    "- We load pre-trained xdistilbert PyTorch model from Huggingface\n",
    "- Convert to onnx representation using torch onnx exporter.\n",
    "- Use TensorRT trtexec command to create the model plan to be hosted with Triton. \n",
    "- The script for exporting this model can be found [here](./workspace/generate_model_trt.sh). \n",
    "\n",
    "Execute the below cell and check out the file for more details\n",
    "\n",
    "<div class=\"alert alert-info\"><strong> Note </strong>\n",
    "This step takes around 10 minutes to complete. While the step is running, please take a look at the logs in the below cell to understand TensorRT optimizations\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefd65d-f654-44b6-9569-0a1d5597a7f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "            -v `pwd`/workspace:/workspace -w /workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_trt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610b257-20d2-4f16-8f05-e482d42608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_trt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d9fdb-b6d0-40fd-bd29-96a53a6a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repository/xdistilbert_trt/config.pbtxt\n",
    "name: \"xdistilbert_trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 224\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"token_type_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [6]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d67ee3-e301-4edb-a6d4-d0b1f35ad2b3",
   "metadata": {},
   "source": [
    "### Export model artifacts to S3 <a class=\"anchor\" id=\"export-to-s3\"></a>\n",
    "\n",
    "SageMaker expects the model artifacts in below format, it should also satisfy Triton container requirements such as model name, version, config.pbtxt files etc. `tar` the folder containing the model file and upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb9432-5c5f-449e-88c3-11d1d842af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_pt/1/\n",
    "!cp -f workspace/model.pt model_repository/xdistilbert_pt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5022a48f-5d52-4697-8b12-f00fc297f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C model_repository -czf $pytorch_model_file_name xdistilbert_pt\n",
    "model_uri_pt = sagemaker_session.upload_data(path=pytorch_model_file_name, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f5a7b9-416e-40aa-be62-d956d52146f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model S3 location: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_pt.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Model S3 location: {model_uri_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f302c9-f4a1-449a-b381-834100100f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/xdistilbert_trt/1/\n",
    "!cp -f workspace/model.plan model_repository/xdistilbert_trt/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296c069a-2fad-4e22-891a-cf0a05920327",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C model_repository -czf $tensorrt_model_file_name xdistilbert_trt\n",
    "model_uri_trt = sagemaker_session.upload_data(path=tensorrt_model_file_name, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d49ab86-fcb1-467b-96a9-4ce518914b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT Model S3 location: s3://sagemaker-us-west-2-354625738399/nlp-mme-gpu/xdistilbert_trt.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorRT Model S3 location: {model_uri_trt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8f41-08e5-4999-98e7-92ee2852940d",
   "metadata": {},
   "source": [
    "### Deploy Models with MME <a class=\"anchor\" id=\"deploy-models-with-mme\"></a>\n",
    "\n",
    "We will now deploy xtreme distilBERT model with different framework backends i.e. PyTorch, TensorRT to SageMaker MME.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Note </strong>\n",
    "you can deploy 1000s of models. The models can use same framework. They can also use different frameworks as shown in this note.\n",
    "</div>\n",
    "\n",
    "We will use AWS SDK for Python (Boto) APIs [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model), [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) and [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint) to create a mulit-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1f6b1-c210-484f-8bfb-4c05f84be0d3",
   "metadata": {},
   "source": [
    "### Define the serving container  <a class=\"anchor\" id=\"define-container-def\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c2773-10f6-4b18-bf8b-6a974ae2daf2",
   "metadata": {},
   "source": [
    " In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load  and serve predictions. Set `Mode` to `MultiModel` to indicates SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU, see MME [container images](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-support) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379000a-3116-4ed4-851b-a041e4cfb3bf",
   "metadata": {},
   "source": [
    "### SageMaker Triton Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25758107-1d7a-4085-8a6a-908844f03ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53b00914-a237-4455-a08c-d79b3715cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri, \"ModelDataUrl\": model_data_url, \"Mode\": \"MultiModel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e28483-e6cf-4769-b6f3-4ccaa61471ec",
   "metadata": {},
   "source": [
    "### Create a MME object <a class=\"anchor\" id=\"create-mme-model-obj\"></a>\n",
    "\n",
    "Using the SageMaker boto3 client, create the model using [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the create model API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2077c860-1ef6-42f6-b10b-c77e075d2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:354625738399:model/nlp-mme-gpu-mdl-2023-01-27-02-53-04\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8e07e-1410-471e-9c2c-10e0c23db00f",
   "metadata": {},
   "source": [
    "### Define configuration for the MME<a class=\"anchor\" id=\"config-mme\"></a>\n",
    "\n",
    "Create a multi-model endpoint configuration using [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) boto3 API. Specify an accelerated GPU computing instance in InstanceType (we will use the same instance type that we are using to host our SageMaker Notebook). We recommend configuring your endpoints with at least two instances with real-life use-cases. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b55e160-8aa0-4ef7-84f5-cb975df99f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint-config/nlp-mme-gpu-epc-2023-01-27-02-53-04\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943aee-7692-4048-9a47-256d8e21c112",
   "metadata": {},
   "source": [
    "### Create MME  <a class=\"anchor\" id=\"create-mme\"></a>\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fc49275-e1c2-46c3-a8be-9e193c271262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/nlp-mme-gpu-ep-2023-01-27-02-53-04\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404e50c-9f66-4b27-9b9a-714f474cc1d9",
   "metadata": {},
   "source": [
    "### Describe MME <a class=\"anchor\" id=\"describe-mme\"></a>\n",
    "\n",
    "Now, we check the status of the endpoint using `describe_endpoint`. This step will take about 7 mins to complete and you should see `Status: InService` message before you proceed to next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d7ac9f8-8ba9-4f37-a2cb-7dc6d428885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/nlp-mme-gpu-ep-2023-01-27-02-53-04\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a0ed6-ff2d-46cb-a396-4beef96b58d9",
   "metadata": {},
   "source": [
    "## Helper functions <a class=\"anchor\" id=\"helper-functions\"></a>\n",
    "\n",
    "The following method transforms a sample test we will be using for inference into the payload that can be sent for inference to the Triton server. These will be used by PyTorch and TensorRT distilbert models.\n",
    "\n",
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "daae74d9-1058-4686-9f36-e297277d26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"emotion\")\n",
    "tokenizer_name = \"bergum/xtremedistil-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_text(tokenizer, text):\n",
    "    MAX_LEN = 128\n",
    "    tokenized_text = tokenizer(text, padding='max_length', max_length=MAX_LEN, add_special_tokens=True, return_tensors='np')\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask, tokenized_text.token_type_ids\n",
    "\n",
    "def _get_tokenized_text_binary(text, input_names, output_names):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_ids, attention_mask, token_type_ids = tokenize_text(tokenizer, text)\n",
    "    inputs.append(httpclient.InferInput(input_names[0], input_ids.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], attention_mask.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[2], token_type_ids.shape, \"INT32\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True)\n",
    "    inputs[2].set_data_from_numpy(token_type_ids.astype(np.int32), binary_data=True)\n",
    "    \n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "def get_random_text():\n",
    "    rand_i = random.randint(0, 2000)\n",
    "    text = dataset[\"test\"][\"text\"][rand_i]\n",
    "    return text\n",
    "\n",
    "def get_tokenized_text_binary_pt(text):\n",
    "    return _get_sample_tokenized_text_binary(text, [\"INPUT__0\", \"INPUT__1\", \"INPUT__2\"], [\"OUTPUT__0\"])\n",
    "\n",
    "\n",
    "def get_tokenized_text_binary_trt(text):\n",
    "    return _get_sample_tokenized_text_binary(text, [\"input_ids\", \"attention_mask\", \"token_type_ids\"], [\"logits\"])\n",
    "\n",
    "def logits2predictions(logits):\n",
    "    CLASSES = [\"SADNESS\", \"JOY\", \"LOVE\", \"ANGER\", \"FEAR\", \"SURPRISE\"]\n",
    "    predictions = []\n",
    "    for i in range(len(logits)):\n",
    "        pred_class_idx = np.argmax(logits[i])\n",
    "        predictions.append(CLASSES[pred_class_idx])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887d53d-d9b4-41bf-ab74-07a86cecd04c",
   "metadata": {},
   "source": [
    "## Invoke Target Models on Multi-Model Endpoint\n",
    "\n",
    "Once the endpoint is successfully created, we can send inference request to multi-model endpoint using invoke_enpoint API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a05c-ae58-484c-9fed-4c9584795c7f",
   "metadata": {},
   "source": [
    "### Invoke PyTorch Model <a class=\"anchor\" id=\"invoke-pytorch-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4ab429b-5b9c-4edd-9ff7-5f974dd35924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANGER']\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_pt(sample_text)\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_pt.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "output_name = \"OUTPUT__0\"\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "logits = result.as_numpy(output_name)\n",
    "predictions = logits2predictions(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41729b83-9921-4f26-b7cf-74f9f81365fc",
   "metadata": {},
   "source": [
    "### Invoke TensorRT Model <a class=\"anchor\" id=\"invoke-tensorrt-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90afb76a-72ac-49b0-ae6d-313976bf94f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JOY']\n"
     ]
    }
   ],
   "source": [
    "sample_text = get_random_text()\n",
    "request_body, header_length = get_tokenized_text_binary_trt(sample_text)\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='xdistilbert_trt.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "output_name = \"logits\"\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "logits = result.as_numpy(output_name)\n",
    "predictions = logits2predictions(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa3d1a-c0f2-4809-a7e2-71056556c864",
   "metadata": {},
   "source": [
    "# Deploying Numerous Models to GPUs using MME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f67b3-0eb6-4ead-8851-c89b2628cb1c",
   "metadata": {},
   "source": [
    "Let's say we are trying to deploy 1000 customer-specific distilBERT models which are a mixture of frequently and infrequently accessed models coming from different frameworks like PyTorch, TensorFlow, ONNX, TensorRT.\n",
    "\n",
    "Deploying these 1000 models on GPU instances like `g5.xlarge` using dedicated Single-Model Endpoints would take ~1000 instances.\n",
    "\n",
    "By leveraging MME on GPU, you can deploy these models behind a single MME endpoint and end up using 100x less instances. \n",
    "\n",
    "Thus reducing costs by **100x**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedb827-b9e7-4b54-98f4-f3effa813bc5",
   "metadata": {},
   "source": [
    "## Dynamically adding models to an existing endpoint\n",
    "\n",
    "It’s easy to deploy a new model to an existing multi-model endpoint. With the endpoint already running, copy a new set of model artifacts to the same S3 location you set up earlier. Client applications are then free to request predictions from that target model, and Amazon SageMaker handles the rest. \n",
    "\n",
    "With multi-model endpoints, you don’t need to go through a full endpoint update just to deploy a new model, and you avoid the cost of a separate endpoint for each new model. An S3 copy is all that is needed to deploy.\n",
    "\n",
    "This step will take around 5 minutes to complete as we are copying 400 files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd99243-9f2e-4954-8ec8-d9b9c8301ddb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_models = 400\n",
    "for i in range(1, num_models+1):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    model_copy = f\"{model_data_url}{customer_model_name}\"\n",
    "    !aws s3 cp $model_data_url$pytorch_model_file_name $model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f719fa-2f05-41b0-a526-7c66916977b1",
   "metadata": {},
   "source": [
    "## Invoking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50249953-fdb7-40d7-be16-4e4314bcde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(text, model_name, show_latency=False):\n",
    "    print(f\"Using model {model_name} to predict\")\n",
    "    \n",
    "    request_body, header_length = get_sample_tokenized_text_binary_pt(text)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel=model_name)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Parse json header size length from the response\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "    output_name = \"OUTPUT__0\"\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response['Body'].read(), header_length=int(header_length_str))\n",
    "    logits = result.as_numpy(output_name)\n",
    "    predictions = get_predictions(logits)\n",
    "    \n",
    "    if show_latency:\n",
    "        print(f\"prediction: {predictions}, took {int(duration * 1000)} ms\\n\")\n",
    "    else:\n",
    "        print(f\"prediction: {predictions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13e63b14-bcef-40e5-93c3-413f9ed17162",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n",
      "prediction: ['JOY'], took 49 ms\n",
      "\n",
      "Using model xdistilbert_customer2.tar.gz to predict\n",
      "prediction: ['JOY'], took 10 ms\n",
      "\n",
      "Using model xdistilbert_customer3.tar.gz to predict\n",
      "prediction: ['JOY'], took 9 ms\n",
      "\n",
      "Using model xdistilbert_customer4.tar.gz to predict\n",
      "prediction: ['JOY'], took 10 ms\n",
      "\n",
      "Using model xdistilbert_customer5.tar.gz to predict\n",
      "prediction: ['FEAR'], took 9 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(get_random_text(), customer_model_name, show_latency=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be487018-d1bc-42dd-9737-fb7d89aa2f76",
   "metadata": {},
   "source": [
    "[Show ModelCacheHit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef26cf-3ca9-4702-88eb-963e794b69f4",
   "metadata": {},
   "source": [
    "# Dynamic Model Unloading Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c35875ae-e2b5-4626-a88e-6b77dc6c6f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer2.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer3.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer4.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer5.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer6.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer7.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer8.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer9.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer10.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer11.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer12.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer13.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer14.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer15.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer16.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer17.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer18.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer19.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer20.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer21.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer22.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer23.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer24.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer25.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer26.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer27.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer28.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer29.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer30.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer31.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer32.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer33.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer34.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer35.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer36.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer37.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer38.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer39.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer40.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer41.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer42.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer43.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer44.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer45.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer46.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer47.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer48.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer49.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer50.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer51.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer52.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer53.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer54.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer55.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer56.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer57.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer58.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer59.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer60.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer61.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer62.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer63.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer64.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer65.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer66.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer67.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer68.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer69.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer70.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer71.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer72.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer73.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer74.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer75.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer76.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer77.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer78.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer79.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer80.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer81.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer82.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer83.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer84.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer85.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer86.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer87.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer88.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer89.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer90.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer91.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer92.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer93.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer94.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer95.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer96.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer97.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer98.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer99.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer100.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer101.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer102.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer103.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer104.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer105.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer106.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer107.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer108.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer109.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer110.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer111.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer112.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer113.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer114.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer115.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer116.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer117.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer118.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer119.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer120.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer121.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer122.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer123.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer124.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer125.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer126.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer127.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer128.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer129.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer130.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer131.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer132.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer133.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer134.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer135.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer136.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer137.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer138.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer139.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer140.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer141.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer142.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer143.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer144.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer145.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer146.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer147.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer148.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer149.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer150.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer151.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer152.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer153.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer154.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer155.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer156.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer157.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer158.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer159.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer160.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer161.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer162.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer163.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer164.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer165.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer166.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer167.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer168.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer169.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer170.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer171.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer172.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer173.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer174.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer175.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer176.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer177.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer178.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer179.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer180.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer181.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer182.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer183.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer184.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer185.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer186.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer187.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer188.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer189.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer190.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer191.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer192.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer193.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer194.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer195.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer196.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer197.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer198.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer199.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer200.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer201.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer202.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer203.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer204.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer205.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer206.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer207.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer208.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer209.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer210.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer211.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer212.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer213.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer214.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer215.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer216.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer217.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer218.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer219.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer220.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer221.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer222.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer223.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer224.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer225.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer226.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer227.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer228.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer229.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer230.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer231.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer232.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer233.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer234.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer235.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer236.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer237.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer238.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer239.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer240.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer241.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer242.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer243.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer244.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer245.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer246.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer247.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer248.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer249.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer250.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer251.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer252.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer253.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer254.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer255.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer256.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer257.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer258.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer259.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer260.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer261.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer262.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer263.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer264.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer265.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer266.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer267.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer268.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer269.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer270.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer271.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer272.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer273.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer274.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer275.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer276.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer277.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer278.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer279.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer280.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer281.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer282.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer283.tar.gz to predict\n",
      "prediction: ['ANGER']\n",
      "\n",
      "Using model xdistilbert_customer284.tar.gz to predict\n",
      "prediction: ['FEAR']\n",
      "\n",
      "Using model xdistilbert_customer285.tar.gz to predict\n",
      "prediction: ['SURPRISE']\n",
      "\n",
      "Using model xdistilbert_customer286.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer287.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer288.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer289.tar.gz to predict\n",
      "prediction: ['LOVE']\n",
      "\n",
      "Using model xdistilbert_customer290.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer291.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer292.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer293.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer294.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer295.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer296.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer297.tar.gz to predict\n",
      "prediction: ['JOY']\n",
      "\n",
      "Using model xdistilbert_customer298.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n",
      "Using model xdistilbert_customer299.tar.gz to predict\n",
      "prediction: ['SADNESS']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 300):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(text=get_random_text(), model_name=customer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985760-04a4-4718-977e-05d12d1e39bf",
   "metadata": {},
   "source": [
    "[Show logs to show unloading]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b8331-282f-4d09-ba75-8893b382f743",
   "metadata": {},
   "source": [
    "[Show LoadedModelCount and GPUMemoryUtilization]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b006088-db0a-45e4-96a7-37e901d17e75",
   "metadata": {},
   "source": [
    "# Autoscaling Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f091ec-f438-41c0-b00a-71f50111b546",
   "metadata": {},
   "source": [
    "## Set up AutoScaling Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f2eb482-42ee-43f0-a7d9-2bb7328a5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_scaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' \n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity = 1,\n",
    "    MaxCapacity = 2\n",
    ")\n",
    "\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName='GPUMemUtil-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 80, \n",
    "        'CustomizedMetricSpecification':\n",
    "        {\n",
    "            'MetricName': 'GPUMemoryUtilization',\n",
    "            'Namespace': '/aws/sagemaker/Endpoints',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                {'Name': 'VariantName','Value': 'AllTraffic'}\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "            'Unit': 'Percent'\n",
    "        },\n",
    "        'ScaleInCooldown': 600,\n",
    "        'ScaleOutCooldown': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Autoscaling policy for GPU MME endpoint has been set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c8ad8-e116-454f-8ccc-cee513ef01ac",
   "metadata": {},
   "source": [
    "[Show CW alarm being triggered and Endpoint entering Updating state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dca6a-93cd-4045-aaa2-857e22a81311",
   "metadata": {},
   "source": [
    "### While Autoscaling the endpoint is still active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef815d0-fc2f-4b0a-80ef-d9d925e78a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "predict_model(sample_text, customer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6014b7-f01f-4cfd-b7a1-43b7db2145d7",
   "metadata": {},
   "source": [
    "[Show Endpoint autoscaling to 2 instances]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5599ff-15b5-4bb3-9b7d-d22a87aab8f9",
   "metadata": {},
   "source": [
    "## Invoke More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46acc588-39d0-453d-8ad7-37cbcbb329d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model xdistilbert_customer1.tar.gz to predict\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "Read timeout on endpoint URL: \"https://runtime.sagemaker.us-west-2.amazonaws.com/endpoints/nlp-mme-gpu-ep-2023-01-27-02-53-04/invocations\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    786\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             raise ReadTimeoutError(\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: AWSHTTPSConnectionPool(host='runtime.sagemaker.us-west-2.amazonaws.com', port=443): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19859/417984806.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcustomer_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"xdistilbert_customer{i}.tar.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_19859/2796820323.py\u001b[0m in \u001b[0;36mpredict_model\u001b[0;34m(text, model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                   \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/vnd.sagemaker-triton.binary+json;json-header-size={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                   \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mapply_request_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             http, parsed_response = self._make_request(\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         )\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         )\n\u001b[0;32m--> 202\u001b[0;31m         while self._needs_retry(\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_needs_retry\u001b[0;34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mevent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"needs-retry.{service_id}.{operation_model.name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         responses = self._event_emitter.emit(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempts, response, caught_exception, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mchecker_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'retries_context'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretries_context\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mchecker_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retry needed, action of: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception, retries_context)\u001b[0m\n\u001b[1;32m    282\u001b[0m             )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         should_retry = self._should_retry(\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_should_retry\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# If we've exceeded the max attempts we just let the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# propogate if one has occurred.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             checker_response = checker(\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcaught_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             return self._check_caught_exception(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_check_caught_exception\u001b[0;34m(self, attempt_number, caught_exception)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# the MaxAttemptsDecorator is not interested in retrying the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;31m# then this exception just propogates out past the retry code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLLib3ReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             raise ConnectionClosedError(\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: Read timeout on endpoint URL: \"https://runtime.sagemaker.us-west-2.amazonaws.com/endpoints/nlp-mme-gpu-ep-2023-01-27-02-53-04/invocations\""
     ]
    }
   ],
   "source": [
    "for i in range(1, 400):\n",
    "    customer_model_name = f\"xdistilbert_customer{i}.tar.gz\"\n",
    "    predict_model(sample_text, customer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbec88f-cbdb-4567-9048-50c0d9746fae",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0f756-6a47-4a8a-970a-32493ddb27f2",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1081fd3f-48c1-4de1-bbfc-4368ae8e8742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8eec899c-6253-440e-bf1a-68cd088616ef',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8eec899c-6253-440e-bf1a-68cd088616ef',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Fri, 27 Jan 2023 05:57:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31407206-2d9e-4cc8-83a7-8d9e8e4748c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
